{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data: points on the curve (x,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.loadtxt('X.txt')\n",
    "D = np.loadtxt('D.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the neural network\n",
    "\n",
    "This neural network contains 3 layers\n",
    "\n",
    "Layer 1 is input layer consists of x and bias point = 1\n",
    "\n",
    "Layer 2 is a hidden layer consists of 24 neurons and the activation function is tanh\n",
    "\n",
    "Layer 3 is the output layer which has 1 neuron and there is no activation function (or assume linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weights for 2nd and 3rd layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Layer_2_Guess = np.loadtxt('W_Layer_2_InitialGuess.txt')\n",
    "W_Layer_3_Guess = np.loadtxt('W_Layer_3_InitialGuess.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ForwardPass(x, W_Layer_2, W_Layer_3):\n",
    "    # v_2 is the pre activation value in layer 2\n",
    "    v_2 = np.dot(W_Layer_2, x)\n",
    "    \n",
    "    # y_1 is the post activation value in layer 2\n",
    "    y_2 = np.tanh(v_2)\n",
    "    \n",
    "    # v_3 is the pre activation value in layer 3 \n",
    "    # add bias term for layer 3\n",
    "    v_3 =  np.dot(W_Layer_3, np.concatenate((np.ones([1,y_2.shape[1]], dtype=float), y_2), axis=0))\n",
    "    \n",
    "    # y_3 is the post activation value in layer 3\n",
    "    y_3 = v_3\n",
    "    \n",
    "    return v_2, v_3, y_2, y_3\n",
    "\n",
    "# example for a single point: \n",
    "# x = np.array([X[:,2]]).T\n",
    "#[v_2, v_3, y_2, y_3] = ForwardPass(x, W_Layer_2, W_Layer_3)\n",
    "\n",
    "# example for entire dataset\n",
    "# [v_2, v_3, y_2, y_3] = ForwardPass(X, W_Layer_2, W_Layer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeedbackError(output, d):\n",
    "    value = (2/300)*(d-output)\n",
    "    return value\n",
    "#example\n",
    "# err = FeedbackError(y_3, D[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation equations\n",
    "\n",
    "$\\delta_L = (-\\frac{dE}{d y_L})\\cdot \\phi^{\\prime}(v_L)$ where $\\cdot$ denotes component wise multiplication and for the spcial case of MSE we obtain $\\delta_L = (d-y_L)\\cdot \\phi^{\\prime}(V_L)$\n",
    "\n",
    "$\\delta_l = \\tilde{W}_{l+1}^T\\delta_{l+1} \\cdot \\phi^{\\prime}(v_l)$ for $l = 1,2,\\ldots, L-1$ where $\\tilde{W}$ is the weight matrix excluding the bias weight\n",
    "\n",
    "Gradient for layer $l$ is given by $\\frac{d E}{d W_l} = - \\delta_l \\begin{bmatrix} 1 \\\\ y_{l-1} \\end{bmatrix}^T$, for $l = 1,2,\\ldots, L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_linear(v):\n",
    "    return np.ones(v.shape, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative_tanh(v):\n",
    "    #numpy.array([1 - np.square(np.tanh(i)) for i in v])\n",
    "    value = np.array(1-np.square(np.tanh(v)))\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BackwardPass(err, x, v_2, v_3, y_2, y_3, W_Layer_3):\n",
    "    delta_3 = err*derivative_linear(v_3)\n",
    "    Gradient_Layer_3 = - np.dot(delta_3, np.concatenate((np.ones([1,y_2.shape[1]], dtype=float), y_2), axis=0).T)\n",
    "    \n",
    "    #delta_2 =  np.array([(W_Layer_3[1:]*delta_3)*derivative_tanh(v_2)]).T\n",
    "    delta_2 = np.array([W_Layer_3[1:]*delta_3]).T*derivative_tanh(v_2)\n",
    "    Gradient_Layer_2 =  -np.dot(delta_2 , x.T)\n",
    "    \n",
    "    return Gradient_Layer_2, Gradient_Layer_3\n",
    "\n",
    "\n",
    "#example\n",
    "#x = np.array([X[:,2]]).T\n",
    "#[v_2, v_3, y_2, y_3] = ForwardPass(X[:,2], W_Layer_2, W_Layer_3)\n",
    "#err = FeedbackError(y_3, D[2])\n",
    "#[Gradient_Layer_2, Gradient_Layer_3] =BackwardPass(err, x, v_2, v_3, y_2, y_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update weights using Backprop\n",
    "\n",
    "Update weights using gradient descent $W_l \\leftarrow W_{l} - \\eta \\frac{dE}{dW_{l}}$ for $l = 1,2, \\ldots, L$\n",
    "\n",
    "that is update weights using the equation $W_l \\leftarrow W_{l} + \\eta \\delta_{l} \\begin{bmatrix} 1\\\\ y_{l-1} \\end{bmatrix}^T$ for $l = 1,2, \\ldots, L$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateWeights_withMomentum(eta, beta, M2, M3, Gradient_Layer_2, Gradient_Layer_3, W_Layer_2, W_Layer_3):\n",
    "    M2 = (beta * M2) - (eta * Gradient_Layer_2) # Momentum\n",
    "    M3 = (beta * M3) - (eta * Gradient_Layer_3) # Momentum\n",
    "    W_Layer_2 = W_Layer_2 + M2\n",
    "    W_Layer_3 = W_Layer_3 + M3\n",
    "    return W_Layer_2, W_Layer_3, M2, M3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UpdateWeights(eta, Gradient_Layer_2, Gradient_Layer_3, W_Layer_2, W_Layer_3):\n",
    "    W_Layer_2 = W_Layer_2 - (eta*Gradient_Layer_2)\n",
    "    W_Layer_3 = W_Layer_3 - (eta*Gradient_Layer_3)\n",
    "    return W_Layer_2, W_Layer_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculates the mean square error after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalculateMSE(X, D, output):\n",
    "    n = len(X.T)\n",
    "    mse = np.sum(np.square(D-output))/n \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reduce the learning if the MSE error overshoots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CheckLearningrate(eta, MSE_vec, iterations):\n",
    "    if MSE_vec[iterations] > MSE_vec[iterations-1]:\n",
    "        eta = 0.4 * eta\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_iter = 1e4 # Maximum Allowed Iterations\n",
    "max_iter = 2500\n",
    "n = len(X.T) # number of samples in training set\n",
    "tol = 0.005 # Terminating Condition\n",
    "M2 = 0 # Momentum Vector for Layer 2\n",
    "M3 = 0 # Momentum Vector for Layer 3\n",
    "beta = 0.9 # Momentum Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  MSE:  0.5489863481025009  Learning Rate:  4 \n",
      "\n",
      "Epoch:  2  MSE:  0.5292952247012037  Learning Rate:  4 \n",
      "\n",
      "Epoch:  3  MSE:  0.5265427348766326  Learning Rate:  4 \n",
      "\n",
      "Epoch:  4  MSE:  0.5286789360138326  Learning Rate:  4 \n",
      "\n",
      "Epoch:  5  MSE:  0.5317595495225776  Learning Rate:  1.6 \n",
      "\n",
      "Epoch:  6  MSE:  0.5036883799173667  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  7  MSE:  0.5029335230617028  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  8  MSE:  0.5023216016109919  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  9  MSE:  0.5018134823106999  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  10  MSE:  0.5013833860861927  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  11  MSE:  0.5010137298112668  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  12  MSE:  0.5006920090561278  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  13  MSE:  0.5004090072094404  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  14  MSE:  0.5001577440328486  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  15  MSE:  0.4999328249979472  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  16  MSE:  0.49973001487228697  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  17  MSE:  0.49954594463705254  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  18  MSE:  0.4993779029956507  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  19  MSE:  0.49922368452225935  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  20  MSE:  0.4990814772271605  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  21  MSE:  0.49894977826600095  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  22  MSE:  0.4988273300890904  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  23  MSE:  0.49871307161574124  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  24  MSE:  0.49860610055915816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  25  MSE:  0.49850564409668446  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  26  MSE:  0.4984110358365452  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  27  MSE:  0.4983216975734143  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  28  MSE:  0.4982371247155314  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  29  MSE:  0.49815687454954427  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  30  MSE:  0.4980805567163041  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  31  MSE:  0.4980078254230196  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  32  MSE:  0.4979383730297252  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  33  MSE:  0.4978719247318116  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  34  MSE:  0.49780823412316694  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  35  MSE:  0.49774707947188046  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  36  MSE:  0.49768826057647697  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  37  MSE:  0.4976315960982175  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  38  MSE:  0.4975769212862544  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  39  MSE:  0.49752408602890236  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  40  MSE:  0.4974729531771785  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  41  MSE:  0.4974233970968917  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  42  MSE:  0.4973753024135806  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  43  MSE:  0.4973285629209929  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  44  MSE:  0.4972830806289212  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  45  MSE:  0.4972387649303416  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  46  MSE:  0.4971955318711512  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  47  MSE:  0.49715330350853815  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  48  MSE:  0.49711200734624866  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  49  MSE:  0.4970715758368711  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  50  MSE:  0.49703194594277295  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  51  MSE:  0.4969930587486045  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  52  MSE:  0.4969548591193287  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  53  MSE:  0.49691729539863266  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  54  MSE:  0.49688031914330527  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  55  MSE:  0.49684388488979303  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  56  MSE:  0.4968079499496646  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  57  MSE:  0.4967724742311477  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  58  MSE:  0.4967374200842685  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  59  MSE:  0.4967027521674164  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  60  MSE:  0.49666843733340577  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  61  MSE:  0.49663444453329303  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  62  MSE:  0.4966007447363624  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  63  MSE:  0.4965673108647951  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  64  MSE:  0.4965341177416057  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  65  MSE:  0.4965011420504685  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  66  MSE:  0.4964683623060421  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  67  MSE:  0.49643575883338653  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  68  MSE:  0.49640331375499014  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  69  MSE:  0.4963710109838429  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  70  MSE:  0.49633883622088226  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  71  MSE:  0.49630677695500963  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  72  MSE:  0.4962748224637313  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  73  MSE:  0.4962429638123325  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  74  MSE:  0.4962111938493571  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  75  MSE:  0.4961795071960216  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  76  MSE:  0.49614790022710975  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  77  MSE:  0.49611637104080986  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  78  MSE:  0.49608491941497185  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  79  MSE:  0.4960535467473078  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  80  MSE:  0.49602225597722  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  81  MSE:  0.49599105148718886  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  82  MSE:  0.4959599389820176  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  83  MSE:  0.4959289253447139  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  84  MSE:  0.4958980184683947  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  85  MSE:  0.4958672270643158  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  86  MSE:  0.4958365604469556  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  87  MSE:  0.4958060282979714  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  88  MSE:  0.4957756404117863  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  89  MSE:  0.49574540642650067  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  90  MSE:  0.4957153355446974  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  91  MSE:  0.4956854362494827  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  92  MSE:  0.495655716021703  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  93  MSE:  0.4956261810646538  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  94  MSE:  0.49559683604271565  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  95  MSE:  0.49556768384016114  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  96  MSE:  0.49553872534588966  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  97  MSE:  0.49550995926905045  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  98  MSE:  0.4954813819894592  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  99  MSE:  0.4954529874454325  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  100  MSE:  0.49542476706024646  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  101  MSE:  0.49539670970694233  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  102  MSE:  0.4953688017097329  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  103  MSE:  0.49534102687891735  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  104  MSE:  0.4953133665750285  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  105  MSE:  0.49528579979702214  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  106  MSE:  0.4952583032886397  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  107  MSE:  0.49523085165674585  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  108  MSE:  0.49520341749535135  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  109  MSE:  0.4951759715092338  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  110  MSE:  0.4951484826314875  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  111  MSE:  0.4951209181299019  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  112  MSE:  0.4950932436977814  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  113  MSE:  0.49506542352555  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  114  MSE:  0.49503742035024556  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  115  MSE:  0.49500919548071315  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  116  MSE:  0.49498070879693207  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  117  MSE:  0.49495191872244104  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  118  MSE:  0.4949227821692284  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  119  MSE:  0.4948932544547343  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  120  MSE:  0.49486328919077766  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  121  MSE:  0.4948328381442525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  122  MSE:  0.4948018510693739  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  123  MSE:  0.49477027551110553  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  124  MSE:  0.4947380565791404  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  125  MSE:  0.4947051366915187  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  126  MSE:  0.4946714552865831  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  127  MSE:  0.4946369485015717  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  128  MSE:  0.494601548815689  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  129  MSE:  0.49456518465501953  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  130  MSE:  0.49452777995613834  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  131  MSE:  0.4944892536847531  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  132  MSE:  0.4944495193051795  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  133  MSE:  0.4944084841959216  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  134  MSE:  0.49436604900612147  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  135  MSE:  0.494322106947173  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  136  MSE:  0.4942765430133997  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  137  MSE:  0.4942292331254396  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  138  MSE:  0.49418004318990805  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  139  MSE:  0.49412882806914943  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  140  MSE:  0.4940754304555767  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  141  MSE:  0.4940196796464354  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  142  MSE:  0.4939613902171076  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  143  MSE:  0.4939003605946836  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  144  MSE:  0.49383637153901266  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  145  MSE:  0.49376918454654345  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  146  MSE:  0.4936985402039782  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  147  MSE:  0.49362415653544145  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  148  MSE:  0.49354572741028013  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  149  MSE:  0.4934629211111356  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  150  MSE:  0.4933753792065384  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  151  MSE:  0.49328271593278034  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  152  MSE:  0.493184518370738  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  153  MSE:  0.4930803478098342  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  154  MSE:  0.4929697428286626  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  155  MSE:  0.49285222479412755  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  156  MSE:  0.4927273066890304  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  157  MSE:  0.4925945064151964  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  158  MSE:  0.49245336596451  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  159  MSE:  0.4923034780581646  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  160  MSE:  0.4921445219406519  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  161  MSE:  0.49197630983947654  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  162  MSE:  0.4917988449545781  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  163  MSE:  0.49161239044598004  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  164  MSE:  0.49141754644317537  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  165  MSE:  0.4912153284004498  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  166  MSE:  0.49100723529143675  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  167  MSE:  0.49079529095757757  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  168  MSE:  0.4905820381580228  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  169  MSE:  0.4903704652406374  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  170  MSE:  0.49016385280311814  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  171  MSE:  0.4899655436414787  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  172  MSE:  0.4897786613523772  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  173  MSE:  0.48960582405235525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  174  MSE:  0.489448909957089  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  175  MSE:  0.4893089237360428  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  176  MSE:  0.4891859873342934  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  177  MSE:  0.48907944605976383  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  178  MSE:  0.4889880537257661  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  179  MSE:  0.48891018907132455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  180  MSE:  0.48884406053300267  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  181  MSE:  0.4887878715283743  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  182  MSE:  0.48873993547017375  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  183  MSE:  0.4886987428711819  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  184  MSE:  0.48866299008358843  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  185  MSE:  0.48863158127172784  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  186  MSE:  0.4886036141066233  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  187  MSE:  0.4885783572262422  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  188  MSE:  0.48855522492274817  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  189  MSE:  0.4885337523697053  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  190  MSE:  0.4885135731444789  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  191  MSE:  0.48849439978017917  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  192  MSE:  0.48847600747100756  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  193  MSE:  0.4884582207264623  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  194  MSE:  0.4884409026203474  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  195  MSE:  0.4884239462363675  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  196  MSE:  0.4884072679249639  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  197  MSE:  0.48839080202603213  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  198  MSE:  0.48837449676194133  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  199  MSE:  0.4883583110553765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  200  MSE:  0.4883422120723293  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  201  MSE:  0.4883261733302531  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  202  MSE:  0.48831017324459014  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  203  MSE:  0.4882941940140309  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  204  MSE:  0.4882782207667064  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  205  MSE:  0.4882622409068723  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  206  MSE:  0.4882462436153175  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  207  MSE:  0.4882302194674293  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  208  MSE:  0.4882141601411755  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  209  MSE:  0.4881980581937168  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  210  MSE:  0.4881819068903453  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  211  MSE:  0.48816570007328325  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  212  MSE:  0.48814943206082534  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  213  MSE:  0.48813309756957146  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  214  MSE:  0.48811669165422167  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  215  MSE:  0.4881002096607334  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  216  MSE:  0.48808364718964675  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  217  MSE:  0.4880670000671523  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  218  MSE:  0.4880502643220642  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  219  MSE:  0.48803343616730044  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  220  MSE:  0.4880165119848179  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  221  MSE:  0.4879994883131972  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  222  MSE:  0.48798236183727134  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  223  MSE:  0.4879651293793403  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  224  MSE:  0.487947787891619  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  225  MSE:  0.48793033444965345  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  226  MSE:  0.48791276624650676  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  227  MSE:  0.48789508058755415  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  228  MSE:  0.4878772748857752  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  229  MSE:  0.4878593466574442  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  230  MSE:  0.48784129351815525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  231  MSE:  0.48782311317912047  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  232  MSE:  0.4878048034437  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  233  MSE:  0.48778636220412847  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  234  MSE:  0.48776778743841004  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  235  MSE:  0.4877490772073564  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  236  MSE:  0.48773022965175195  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  237  MSE:  0.4877112429896268  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  238  MSE:  0.4876921155136278  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  239  MSE:  0.4876728455884713  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  240  MSE:  0.48765343164847214  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  241  MSE:  0.48763387219513826  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  242  MSE:  0.4876141657948235  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  243  MSE:  0.4875943110764313  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  244  MSE:  0.48757430672916846  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  245  MSE:  0.48755415150033665  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  246  MSE:  0.4875338441931667  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  247  MSE:  0.4875133836646834  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  248  MSE:  0.48749276882360687  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  249  MSE:  0.48747199862828067  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  250  MSE:  0.4874510720846337  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  251  MSE:  0.48742998824416495  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  252  MSE:  0.4874087462019617  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  253  MSE:  0.4873873450947421  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  254  MSE:  0.4873657840989275  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  255  MSE:  0.48734406242874495  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  256  MSE:  0.4873221793343614  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  257  MSE:  0.4873001341000467  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  258  MSE:  0.48727792604237663  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  259  MSE:  0.4872555545084682  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  260  MSE:  0.48723301887425596  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  261  MSE:  0.4872103185428094  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  262  MSE:  0.4871874529426964  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  263  MSE:  0.48716442152639144  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  264  MSE:  0.48714122376873603  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  265  MSE:  0.4871178591654523  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  266  MSE:  0.4870943272317108  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  267  MSE:  0.4870706275007612  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  268  MSE:  0.4870467595226205  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  269  MSE:  0.4870227228628308  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  270  MSE:  0.4869985171012812  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  271  MSE:  0.48697414183110027  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  272  MSE:  0.4869495966576228  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  273  MSE:  0.48692488119742827  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  274  MSE:  0.4868999950774577  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  275  MSE:  0.48687493793420716  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  276  MSE:  0.4868497094129998  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  277  MSE:  0.4868243091673399  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  278  MSE:  0.4867987368583463  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  279  MSE:  0.48677299215426606  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  280  MSE:  0.48674707473007167  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  281  MSE:  0.4867209842671365  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  282  MSE:  0.48669472045299206  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  283  MSE:  0.486668282981162  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  284  MSE:  0.4866416715510777  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  285  MSE:  0.48661488586806484  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  286  MSE:  0.48658792564340747  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  287  MSE:  0.48656079059448243  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  288  MSE:  0.4865334804449618  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  289  MSE:  0.48650599492508195  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  290  MSE:  0.4864783337719759  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  291  MSE:  0.48645049673006324  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  292  MSE:  0.4864224835514966  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  293  MSE:  0.48639429399665973  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  294  MSE:  0.4863659278347124  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  295  MSE:  0.48633738484417866  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  296  MSE:  0.4863086648135756  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  297  MSE:  0.4862797675420733  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  298  MSE:  0.48625069284018746  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  299  MSE:  0.4862214405304968  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  300  MSE:  0.4861920104483795  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  301  MSE:  0.48616240244276915  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  302  MSE:  0.48613261637691807  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  303  MSE:  0.48610265212917153  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  304  MSE:  0.4860725095937418  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  305  MSE:  0.4860421886814832  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  306  MSE:  0.4860116893206583  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  307  MSE:  0.48598101145769723  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  308  MSE:  0.485950155057942  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  309  MSE:  0.4859191201063726  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  310  MSE:  0.485887906608313  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  311  MSE:  0.48585651459011464  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  312  MSE:  0.48582494409980936  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  313  MSE:  0.48579319520773445  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  314  MSE:  0.48576126800712416  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  315  MSE:  0.4857291626146669  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  316  MSE:  0.48569687917102317  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  317  MSE:  0.4856644178413068  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  318  MSE:  0.48563177881552416  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  319  MSE:  0.485598962308969  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  320  MSE:  0.48556596856257783  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  321  MSE:  0.48553279784323705  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  322  MSE:  0.48549945044404647  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  323  MSE:  0.4854659266845365  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  324  MSE:  0.4854322269108393  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  325  MSE:  0.4853983514958132  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  326  MSE:  0.48536430083912097  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  327  MSE:  0.48533007536726075  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  328  MSE:  0.4852956755335532  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  329  MSE:  0.48526110181808196  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  330  MSE:  0.48522635472758907  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  331  MSE:  0.48519143479532756  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  332  MSE:  0.4851563425808691  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  333  MSE:  0.4851210786698738  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  334  MSE:  0.4850856436738128  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  335  MSE:  0.4850500382296573  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  336  MSE:  0.4850142629995251  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  337  MSE:  0.48497831867029123  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  338  MSE:  0.4849422059531639  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  339  MSE:  0.4849059255832233  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  340  MSE:  0.48486947831893107  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  341  MSE:  0.4848328649416045  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  342  MSE:  0.4847960862548637  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  343  MSE:  0.48475914308404955  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  344  MSE:  0.48472203627561233  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  345  MSE:  0.4846847666964798  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  346  MSE:  0.4846473352333962  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  347  MSE:  0.48460974279224067  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  348  MSE:  0.4845719902973255  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  349  MSE:  0.4845340786906733  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  350  MSE:  0.48449600893127676  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  351  MSE:  0.4844577819943401  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  352  MSE:  0.4844193988705079  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  353  MSE:  0.48438086056507473  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  354  MSE:  0.484342168097187  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  355  MSE:  0.4843033224990283  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  356  MSE:  0.4842643248149971  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  357  MSE:  0.48422517610087445  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  358  MSE:  0.4841858774229799  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  359  MSE:  0.48414642985732487  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  360  MSE:  0.48410683448875524  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  361  MSE:  0.4840670924100903  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  362  MSE:  0.484027204721257  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  363  MSE:  0.4839871725284178  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  364  MSE:  0.48394699694309923  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  365  MSE:  0.48390667908131413  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  366  MSE:  0.48386622006268454  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  367  MSE:  0.48382562100956295  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  368  MSE:  0.4837848830461509  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  369  MSE:  0.4837440072976209  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  370  MSE:  0.4837029948892361  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  371  MSE:  0.4836618469454725  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  372  MSE:  0.48362056458914066  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  373  MSE:  0.4835791489405103  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  374  MSE:  0.48353760111643784  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  375  MSE:  0.48349592222949345  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  376  MSE:  0.48345411338709293  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  377  MSE:  0.48341217569063094  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  378  MSE:  0.48337011023461857  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  379  MSE:  0.48332791810582176  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  380  MSE:  0.48328560038240426  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  381  MSE:  0.4832431581330743  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  382  MSE:  0.4832005924162313  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  383  MSE:  0.4831579042791206  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  384  MSE:  0.4831150947569853  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  385  MSE:  0.48307216487222765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  386  MSE:  0.4830291156335672  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  387  MSE:  0.4829859480352055  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  388  MSE:  0.48294266305599176  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  389  MSE:  0.48289926165859126  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  390  MSE:  0.4828557447886565  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  391  MSE:  0.482812113373998  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  392  MSE:  0.4827683683237596  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  393  MSE:  0.4827245105275915  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  394  MSE:  0.48268054085482787  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  395  MSE:  0.48263646015366213  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  396  MSE:  0.48259226925032217  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  397  MSE:  0.4825479689482485  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  398  MSE:  0.4825035600272662  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  399  MSE:  0.48245904324276156  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  400  MSE:  0.4824144193248505  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  401  MSE:  0.48236968897755056  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  402  MSE:  0.4823248528779458  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  403  MSE:  0.4822799116753499  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  404  MSE:  0.4822348659904649  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  405  MSE:  0.48218971641453434  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  406  MSE:  0.4821444635084929  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  407  MSE:  0.4820991078021073  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  408  MSE:  0.4820536497931118  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  409  MSE:  0.48200808994633765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  410  MSE:  0.4819624286928306  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  411  MSE:  0.481916666428962  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  412  MSE:  0.48187080351552974  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  413  MSE:  0.4818248402768482  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  414  MSE:  0.4817787769998262  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  415  MSE:  0.48173261393303185  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  416  MSE:  0.48168635128574816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  417  MSE:  0.4816399892270084  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  418  MSE:  0.48159352788462095  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  419  MSE:  0.48154696734417607  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  420  MSE:  0.481500307648037  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  421  MSE:  0.4814535487943107  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  422  MSE:  0.4814066907358018  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  423  MSE:  0.4813597333789456  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  424  MSE:  0.4813126765827197  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  425  MSE:  0.4812655201575337  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  426  MSE:  0.4812182638640945  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  427  MSE:  0.48117090741224994  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  428  MSE:  0.4811234504598034  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  429  MSE:  0.48107589261130346  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  430  MSE:  0.48102823341680523  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  431  MSE:  0.4809804723706033  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  432  MSE:  0.480932608909932  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  433  MSE:  0.48088464241363754  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  434  MSE:  0.48083657220081316  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  435  MSE:  0.4807883975294036  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  436  MSE:  0.4807401175947723  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  437  MSE:  0.48069173152823214  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  438  MSE:  0.48064323839553824  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  439  MSE:  0.4805946371953409  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  440  MSE:  0.4805459268575991  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  441  MSE:  0.48049710624194936  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  442  MSE:  0.48044817413603447  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  443  MSE:  0.48039912925378286  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  444  MSE:  0.48034997023364817  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  445  MSE:  0.4803006956367941  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  446  MSE:  0.48025130394523446  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  447  MSE:  0.48020179355992326  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  448  MSE:  0.4801521627987915  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  449  MSE:  0.4801024098947296  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  450  MSE:  0.4800525329935189  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  451  MSE:  0.4800025301517053  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  452  MSE:  0.4799523993344161  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  453  MSE:  0.4799021384131186  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  454  MSE:  0.4798517451633187  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  455  MSE:  0.47980121726220093  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  456  MSE:  0.4797505522862037  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  457  MSE:  0.4796997477085326  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  458  MSE:  0.4796488008966105  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  459  MSE:  0.47959770910946287  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  460  MSE:  0.4795464694950332  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  461  MSE:  0.4794950790874378  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  462  MSE:  0.47944353480414764  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  463  MSE:  0.47939183344310454  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  464  MSE:  0.4793399716797678  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  465  MSE:  0.47928794606409253  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  466  MSE:  0.4792357530174354  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  467  MSE:  0.4791833888293947  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  468  MSE:  0.4791308496545776  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  469  MSE:  0.4790781315092989  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  470  MSE:  0.4790252302682087  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  471  MSE:  0.47897214166085406  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  472  MSE:  0.4789188612681708  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  473  MSE:  0.4788653845189057  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  474  MSE:  0.47881170668597833  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  475  MSE:  0.4787578228827713  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  476  MSE:  0.4787037280593618  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  477  MSE:  0.4786494169986925  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  478  MSE:  0.47859488431267894  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  479  MSE:  0.478540124438267  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  480  MSE:  0.47848513163343115  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  481  MSE:  0.47842989997312735  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  482  MSE:  0.4783744233451961  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  483  MSE:  0.47831869544622624  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  484  MSE:  0.4782627097773811  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  485  MSE:  0.4782064596401873  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  486  MSE:  0.4781499381323038  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  487  MSE:  0.4780931381432642  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  488  MSE:  0.47803605235020863  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  489  MSE:  0.477978673213604  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  490  MSE:  0.47792099297296714  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  491  MSE:  0.4778630036425977  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  492  MSE:  0.4778046970073231  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  493  MSE:  0.47774606461827757  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  494  MSE:  0.4776870977887116  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  495  MSE:  0.4776277875898513  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  496  MSE:  0.47756812484682065  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  497  MSE:  0.47750810013462974  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  498  MSE:  0.47744770377425017  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  499  MSE:  0.4773869258287891  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  500  MSE:  0.4773257560997765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  501  MSE:  0.4772641841235766  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  502  MSE:  0.4772021991679482  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  503  MSE:  0.4771397902287636  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  504  MSE:  0.47707694602690426  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  505  MSE:  0.47701365500535453  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  506  MSE:  0.4769499053265141  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  507  MSE:  0.4768856848697391  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  508  MSE:  0.4768209812291446  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  509  MSE:  0.47675578171168487  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  510  MSE:  0.47669007333552843  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  511  MSE:  0.4766238428287602  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  512  MSE:  0.47655707662842445  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  513  MSE:  0.4764897608799412  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  514  MSE:  0.47642188143691155  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  515  MSE:  0.4763534238613452  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  516  MSE:  0.4762843734243292  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  517  MSE:  0.47621471510716445  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  518  MSE:  0.4761444336030005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  519  MSE:  0.4760735133189883  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  520  MSE:  0.4760019383789783  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  521  MSE:  0.47592969262679685  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  522  MSE:  0.4758567596301148  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  523  MSE:  0.4757831226849436  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  524  MSE:  0.47570876482077923  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  525  MSE:  0.47563366880642177  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  526  MSE:  0.47555781715649287  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  527  MSE:  0.4754811921386715  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  528  MSE:  0.4754037757816776  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  529  MSE:  0.47532554988401843  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  530  MSE:  0.4752464960235196  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  531  MSE:  0.47516659556766133  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  532  MSE:  0.475085829684733  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  533  MSE:  0.47500417935582456  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  534  MSE:  0.47492162538766874  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  535  MSE:  0.4748381484263385  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  536  MSE:  0.47475372897181917  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  537  MSE:  0.47466834739345426  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  538  MSE:  0.47458198394626844  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  539  MSE:  0.4744946187881756  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  540  MSE:  0.4744062319980636  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  541  MSE:  0.47431680359475226  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  542  MSE:  0.47422631355682027  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  543  MSE:  0.47413474184328397  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  544  MSE:  0.47404206841512037  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  545  MSE:  0.47394827325760447  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  546  MSE:  0.47385333640345473  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  547  MSE:  0.4737572379567475  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  548  MSE:  0.4736599581175751  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  549  MSE:  0.4735614772074185  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  550  MSE:  0.4734617756951902  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  551  MSE:  0.473360834223913  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  552  MSE:  0.4732586336379846  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  553  MSE:  0.473155155010982  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  554  MSE:  0.47305037967395464  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  555  MSE:  0.4729442892441468  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  556  MSE:  0.4728368656540919  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  557  MSE:  0.4727280911810189  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  558  MSE:  0.4726179484764991  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  559  MSE:  0.47250642059626613  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  560  MSE:  0.47239349103014555  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  561  MSE:  0.47227914373200464  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  562  MSE:  0.47216336314966023  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  563  MSE:  0.47204613425465825  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  564  MSE:  0.47192744257184793  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  565  MSE:  0.4718072742086669  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  566  MSE:  0.4716856158840578  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  567  MSE:  0.4715624549569298  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  568  MSE:  0.47143777945408527  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  569  MSE:  0.4713115780975223  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  570  MSE:  0.47118384033103894  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  571  MSE:  0.471054556346046  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  572  MSE:  0.47092371710651815  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  573  MSE:  0.47079131437299376  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  574  MSE:  0.47065734072555493  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  575  MSE:  0.47052178958570656  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  576  MSE:  0.4703846552370853  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  577  MSE:  0.4702459328449246  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  578  MSE:  0.4701056184742178  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  579  MSE:  0.46996370910650836  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  580  MSE:  0.46982020265525504  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  581  MSE:  0.4696750979797173  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  582  MSE:  0.46952839489730774  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  583  MSE:  0.4693800941943755  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  584  MSE:  0.4692301976353721  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  585  MSE:  0.4690787079703758  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  586  MSE:  0.4689256289409339  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  587  MSE:  0.46877096528421625  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  588  MSE:  0.4686147227354468  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  589  MSE:  0.4684569080286121  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  590  MSE:  0.46829752889544046  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  591  MSE:  0.46813659406264607  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  592  MSE:  0.4679741132474523  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  593  MSE:  0.4678100971513992  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  594  MSE:  0.4676445574524566  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  595  MSE:  0.46747750679546585  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  596  MSE:  0.4673089587809335  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  597  MSE:  0.4671389279522208  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  598  MSE:  0.4669674297811544  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  599  MSE:  0.46679448065211193  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  600  MSE:  0.4666200978446266  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  601  MSE:  0.46644429951455946  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  602  MSE:  0.4662671046739041  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  603  MSE:  0.46608853316927196  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  604  MSE:  0.4659086056591309  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  605  MSE:  0.4657273435898586  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  606  MSE:  0.4655447691706797  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  607  MSE:  0.4653609053475567  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  608  MSE:  0.4651757757761141  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  609  MSE:  0.4649894047936607  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  610  MSE:  0.4648018173903932  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  611  MSE:  0.46461303917985525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  612  MSE:  0.46442309636873175  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  613  MSE:  0.46423201572605327  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  614  MSE:  0.4640398245518921  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  615  MSE:  0.4638465506456229  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  616  MSE:  0.4636522222738327  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  617  MSE:  0.46345686813794695  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  618  MSE:  0.4632605173416515  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  619  MSE:  0.4630631993581901  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  620  MSE:  0.46286494399759115  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  621  MSE:  0.4626657813739185  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  622  MSE:  0.4624657418725902  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  623  MSE:  0.46226485611784807  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  624  MSE:  0.4620631549404307  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  625  MSE:  0.46186066934551717  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  626  MSE:  0.4616574304809929  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  627  MSE:  0.4614534696060978  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  628  MSE:  0.46124881806050516  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  629  MSE:  0.4610435072338803  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  630  MSE:  0.4608375685359687  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  631  MSE:  0.460631033367247  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  632  MSE:  0.460423933090187  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  633  MSE:  0.4602162990011592  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  634  MSE:  0.46000816230301683  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  635  MSE:  0.45979955407838313  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  636  MSE:  0.4595905052636698  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  637  MSE:  0.4593810466238576  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  638  MSE:  0.45917120872804296  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  639  MSE:  0.4589610219257889  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  640  MSE:  0.4587505163242715  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  641  MSE:  0.45853972176625635  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  642  MSE:  0.45832866780889314  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  643  MSE:  0.4581173837033481  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  644  MSE:  0.4579058983752708  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  645  MSE:  0.4576942404060975  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  646  MSE:  0.45748243801519245  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  647  MSE:  0.4572705190428167  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  648  MSE:  0.45705851093392397  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  649  MSE:  0.4568464407227742  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  650  MSE:  0.45663433501835454  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  651  MSE:  0.45642221999059096  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  652  MSE:  0.4562101213573489  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  653  MSE:  0.4559980643721942  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  654  MSE:  0.45578607381290887  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  655  MSE:  0.4555741739707321  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  656  MSE:  0.45536238864032336  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  657  MSE:  0.45515074111041076  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  658  MSE:  0.4549392541551156  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  659  MSE:  0.4547279500259238  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  660  MSE:  0.4545168504442852  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  661  MSE:  0.45430597659481653  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  662  MSE:  0.4540953491190809  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  663  MSE:  0.45388498810992817  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  664  MSE:  0.4536749131063555  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  665  MSE:  0.45346514308888203  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  666  MSE:  0.453255696475395  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  667  MSE:  0.453046591117455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  668  MSE:  0.45283784429702356  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  669  MSE:  0.4526294727236015  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  670  MSE:  0.4524214925317346  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  671  MSE:  0.4522139192788774  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  672  MSE:  0.45200676794358485  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  673  MSE:  0.45180005292400227  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  674  MSE:  0.45159378803663724  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  675  MSE:  0.45138798651538387  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  676  MSE:  0.451182661010783  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  677  MSE:  0.45097782358948774  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  678  MSE:  0.4507734857339195  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  679  MSE:  0.4505696583420885  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  680  MSE:  0.45036635172756545  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  681  MSE:  0.45016357561957626  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  682  MSE:  0.4499613391632088  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  683  MSE:  0.4497596509197122  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  684  MSE:  0.4495585188668722  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  685  MSE:  0.44935795039944615  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  686  MSE:  0.4491579523296493  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  687  MSE:  0.4489585308876763  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  688  MSE:  0.4487596917222447  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  689  MSE:  0.4485614399011581  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  690  MSE:  0.4483637799118757  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  691  MSE:  0.4481667156620858  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  692  MSE:  0.44797025048027744  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  693  MSE:  0.4477743871163045  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  694  MSE:  0.44757912774195413  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  695  MSE:  0.447384473951505  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  696  MSE:  0.4471904267622925  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  697  MSE:  0.4469969866152831  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  698  MSE:  0.4468041533756631  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  699  MSE:  0.4466119263334633  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  700  MSE:  0.446420304204218  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  701  MSE:  0.44622928512969506  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  702  MSE:  0.44603886667869924  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  703  MSE:  0.4458490458479826  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  704  MSE:  0.445659819063283  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  705  MSE:  0.44547118218052123  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  706  MSE:  0.4452831304871919  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  707  MSE:  0.4450956587039742  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  708  MSE:  0.4449087609866164  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  709  MSE:  0.44472243092812147  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  710  MSE:  0.4445366615612907  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  711  MSE:  0.44435144536166293  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  712  MSE:  0.4441667742509183  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  713  MSE:  0.44398263960077816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  714  MSE:  0.4437990322374828  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  715  MSE:  0.44361594244688923  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  716  MSE:  0.4434333599802631  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  717  MSE:  0.443251274060825  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  718  MSE:  0.44306967339111575  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  719  MSE:  0.44288854616125606  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  720  MSE:  0.4427078800581556  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  721  MSE:  0.4425276622757542  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  722  MSE:  0.4423478795263489  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  723  MSE:  0.44216851805307994  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  724  MSE:  0.44198956364363523  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  725  MSE:  0.4418110016452283  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  726  MSE:  0.4416328169809091  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  727  MSE:  0.4414549941672474  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  728  MSE:  0.441277517333431  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  729  MSE:  0.44110037024180815  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  730  MSE:  0.44092353630989123  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  731  MSE:  0.4407469986338245  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  732  MSE:  0.4405707400133131  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  733  MSE:  0.4403947429779841  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  734  MSE:  0.4402189898151342  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  735  MSE:  0.4400434625988074  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  736  MSE:  0.43986814322011186  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  737  MSE:  0.4396930134186698  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  738  MSE:  0.43951805481507117  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  739  MSE:  0.4393432489441642  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  740  MSE:  0.4391685772890008  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  741  MSE:  0.4389940213152198  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  742  MSE:  0.43881956250562043  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  743  MSE:  0.4386451823946502  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  744  MSE:  0.43847086260250384  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  745  MSE:  0.43829658486849044  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  746  MSE:  0.43812233108330967  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  747  MSE:  0.4379480833198348  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  748  MSE:  0.43777382386198915  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  749  MSE:  0.4375995352312602  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  750  MSE:  0.43742520021038644  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  751  MSE:  0.4372508018637296  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  752  MSE:  0.4370763235538132  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  753  MSE:  0.43690174895352146  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  754  MSE:  0.4367270620534104  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  755  MSE:  0.4365522471635995  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  756  MSE:  0.4363772889096967  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  757  MSE:  0.43620217222220464  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  758  MSE:  0.4360268823188621  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  759  MSE:  0.43585140467937294  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  760  MSE:  0.4356757250119718  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  761  MSE:  0.43549982921128094  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  762  MSE:  0.4353237033068988  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  763  MSE:  0.43514733340216005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  764  MSE:  0.43497070560248136  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  765  MSE:  0.4347938059326834  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  766  MSE:  0.4346166202426185  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  767  MSE:  0.4344391341003929  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  768  MSE:  0.4342613326723591  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  769  MSE:  0.4340832005889457  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  770  MSE:  0.43390472179523043  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  771  MSE:  0.4337258793849552  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  772  MSE:  0.4335466554164235  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  773  MSE:  0.43336703070838195  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  774  MSE:  0.4331869846135825  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  775  MSE:  0.43300649476719355  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  776  MSE:  0.4328255368065797  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  777  MSE:  0.43264408405818033  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  778  MSE:  0.4324621071861971  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  779  MSE:  0.43227957379657217  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  780  MSE:  0.43209644798818353  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  781  MSE:  0.4319126898412431  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  782  MSE:  0.43172825483045413  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  783  MSE:  0.4315430931474097  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  784  MSE:  0.431357148912842  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  785  MSE:  0.4311703592543709  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  786  MSE:  0.4309826532190791  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  787  MSE:  0.4307939504820907  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  788  MSE:  0.4306041598017773  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  789  MSE:  0.43041317715847904  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  790  MSE:  0.4302208834956332  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  791  MSE:  0.43002714195849134  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  792  MSE:  0.4298317944941903  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  793  MSE:  0.4296346576350797  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  794  MSE:  0.4294355172311045  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  795  MSE:  0.42923412182151355  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  796  MSE:  0.42903017423396855  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  797  MSE:  0.428823320860286  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  798  MSE:  0.42861313786892236  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  799  MSE:  0.428399113356041  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  800  MSE:  0.4281806240844287  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  801  MSE:  0.42795690497914896  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  802  MSE:  0.427727008898149  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  803  MSE:  0.42748975332408873  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  804  MSE:  0.42724364947671395  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  805  MSE:  0.4269868078823371  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  806  MSE:  0.4267168126675483  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  807  MSE:  0.4264305549025328  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  808  MSE:  0.4261240136064538  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  809  MSE:  0.42579197244840555  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  810  MSE:  0.4254276624867285  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  811  MSE:  0.42502232946364293  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  812  MSE:  0.4245647425061377  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  813  MSE:  0.4240406942846557  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  814  MSE:  0.4234325928205715  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  815  MSE:  0.4227193057254634  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  816  MSE:  0.4218764669154311  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  817  MSE:  0.4208774569162433  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  818  MSE:  0.41969518705033515  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  819  MSE:  0.4183046594032363  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  820  MSE:  0.41668610014443447  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  821  MSE:  0.4148283521062779  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  822  MSE:  0.41273217459734524  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  823  MSE:  0.41041305077856094  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  824  MSE:  0.4079029668229876  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  825  MSE:  0.40525046324021  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  826  MSE:  0.402518273180562  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  827  MSE:  0.3997782491257736  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  828  MSE:  0.39710403088028406  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  829  MSE:  0.3945627503269423  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  830  MSE:  0.3922075812653132  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  831  MSE:  0.3900728088365734  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  832  MSE:  0.3881723243810947  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  833  MSE:  0.38650140696189034  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  834  MSE:  0.38504080958972176  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  835  MSE:  0.3837618269020178  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  836  MSE:  0.38263117627039367  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  837  MSE:  0.3816149578362485  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  838  MSE:  0.38068142237548036  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  839  MSE:  0.379802614033788  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  840  MSE:  0.37895512672183584  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  841  MSE:  0.37812025035100816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  842  MSE:  0.3772837424659403  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  843  MSE:  0.3764353923195688  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  844  MSE:  0.3755684808342087  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  845  MSE:  0.3746791964210483  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  846  MSE:  0.37376604465232016  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  847  MSE:  0.3728292819429881  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  848  MSE:  0.3718703998176752  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  849  MSE:  0.37089168034269815  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  850  MSE:  0.36989583395692494  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  851  MSE:  0.3688857211645301  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  852  MSE:  0.3678641522711468  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  853  MSE:  0.36683375523216566  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  854  MSE:  0.3657968997619023  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  855  MSE:  0.36475566511593716  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  856  MSE:  0.36371183911144583  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  857  MSE:  0.3626669370753704  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  858  MSE:  0.361622231420605  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  859  MSE:  0.36057878503864754  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  860  MSE:  0.359537484140629  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  861  MSE:  0.35849906818550165  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  862  MSE:  0.3574641559463001  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  863  MSE:  0.35643326761896504  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  864  MSE:  0.35540684330845485  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  865  MSE:  0.35438525838504215  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  866  MSE:  0.35336883621240117  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  867  MSE:  0.3523578586901561  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  868  MSE:  0.35135257497347233  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  869  MSE:  0.35035320865410136  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  870  MSE:  0.3493599636200567  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  871  MSE:  0.3483730287573406  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  872  MSE:  0.3473925816165577  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  873  MSE:  0.3464187911383733  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  874  MSE:  0.34545181951328  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  875  MSE:  0.34449182324147354  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  876  MSE:  0.34353895345622393  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  877  MSE:  0.3425933555777511  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  878  MSE:  0.3416551683724006  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  879  MSE:  0.34072452250249574  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  880  MSE:  0.3398015386639672  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  881  MSE:  0.3388863254199747  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  882  MSE:  0.33797897684806194  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  883  MSE:  0.3370795701243473  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  884  MSE:  0.33618816317009437  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  885  MSE:  0.33530479248286665  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  886  MSE:  0.33442947126620387  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  887  MSE:  0.33356218795840364  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  888  MSE:  0.3327029052429386  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  889  MSE:  0.33185155960130736  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  890  MSE:  0.3310080614445346  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  891  MSE:  0.3301722958335429  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  892  MSE:  0.32934412377242783  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  893  MSE:  0.32852338403375003  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  894  MSE:  0.3277098954524306  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  895  MSE:  0.3269034596058675  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  896  MSE:  0.32610386378334816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  897  MSE:  0.32531088413816767  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  898  MSE:  0.3245242889115397  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  899  MSE:  0.3237438416182508  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  900  MSE:  0.32296930408980756  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  901  MSE:  0.3222004392810222  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  902  MSE:  0.32143701375979583  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  903  MSE:  0.3206787998163897  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  904  MSE:  0.3199255771466707  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  905  MSE:  0.31917713408273785  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  906  MSE:  0.31843326836290214  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  907  MSE:  0.31769378745033405  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  908  MSE:  0.31695850842504963  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  909  MSE:  0.3162272574867173  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  910  MSE:  0.3154998691156022  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  911  MSE:  0.31477618494571763  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  912  MSE:  0.3140560524078661  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  913  MSE:  0.31333932320104785  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  914  MSE:  0.3126258516489407  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  915  MSE:  0.31191549299432486  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  916  MSE:  0.31120810167899626  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  917  MSE:  0.3105035296503628  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  918  MSE:  0.3098016247291747  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  919  MSE:  0.30910222906602225  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  920  MSE:  0.30840517770798254  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  921  MSE:  0.3077102972911126  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  922  MSE:  0.3070174048697892  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  923  MSE:  0.30632630689019086  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  924  MSE:  0.3056367983124816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  925  MSE:  0.3049486618844521  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  926  MSE:  0.3042616675684365  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  927  MSE:  0.30357557212289704  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  928  MSE:  0.3028901188402932  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  929  MSE:  0.30220503744324145  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  930  MSE:  0.3015200441416137  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  931  MSE:  0.3008348418538627  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  932  MSE:  0.3001491205964168  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  933  MSE:  0.2994625580454278  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  934  MSE:  0.2987748202753355  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  935  MSE:  0.29808556267867364  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  936  MSE:  0.2973944310711918  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  937  MSE:  0.2967010629857309  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  938  MSE:  0.29600508915730966  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  939  MSE:  0.29530613520057786  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  940  MSE:  0.29460382347911546  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  941  MSE:  0.2938977751640781  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  942  MSE:  0.29318761247724634  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  943  MSE:  0.29247296111087157  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  944  MSE:  0.29175345281355247  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  945  MSE:  0.2910287281280237  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  946  MSE:  0.29029843926299403  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  947  MSE:  0.28956225307727335  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  948  MSE:  0.2888198541503381  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  949  MSE:  0.28807094790932697  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  950  MSE:  0.28731526377837974  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  951  MSE:  0.2865525583122905  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  952  MSE:  0.28578261827288853  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  953  MSE:  0.28500526360340617  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  954  MSE:  0.28422035025369596  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  955  MSE:  0.2834277728074525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  956  MSE:  0.2826274668619819  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  957  MSE:  0.28181941111148284  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  958  MSE:  0.2810036290864869  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  959  MSE:  0.28018019050514437  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  960  MSE:  0.27934921219636305  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  961  MSE:  0.2785108585605845  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  962  MSE:  0.2776653415409621  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  963  MSE:  0.2768129200859101  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  964  MSE:  0.27595389909318035  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  965  MSE:  0.2750886278355379  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  966  MSE:  0.2742174978785406  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  967  MSE:  0.27334094051147484  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  968  MSE:  0.27245942372287724  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  969  MSE:  0.27157344876191075  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  970  MSE:  0.2706835463357757  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  971  MSE:  0.2697902725010808  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  972  MSE:  0.26889420431335526  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  973  MSE:  0.267995935303385  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  974  MSE:  0.2670960708517684  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  975  MSE:  0.26619522353381647  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  976  MSE:  0.26529400850575513  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  977  MSE:  0.26439303900014255  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  978  MSE:  0.2634929219937208  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  979  MSE:  0.2625942541047238  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  980  MSE:  0.26169761776927175  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  981  MSE:  0.2608035777381896  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  982  MSE:  0.259912677926721  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  983  MSE:  0.2590254386404935  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  984  MSE:  0.25814235419199644  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  985  MSE:  0.25726389091314594  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  986  MSE:  0.2563904855613421  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  987  MSE:  0.2555225441091217  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  988  MSE:  0.2546604409010817  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  989  MSE:  0.2538045181564338  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  990  MSE:  0.25295508579131637  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  991  MSE:  0.2521124215318163  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  992  MSE:  0.25127677128665504  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  993  MSE:  0.25044834974736024  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  994  MSE:  0.24962734118365015  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  995  MSE:  0.24881390040233423  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  996  MSE:  0.2480081538393394  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  997  MSE:  0.24721020075626673  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  998  MSE:  0.24642011451509926  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  999  MSE:  0.24563794390711413  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1000  MSE:  0.24486371451471597  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1001  MSE:  0.24409743008754475  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1002  MSE:  0.24333907391686457  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1003  MSE:  0.2425886101947923  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1004  MSE:  0.2418459853472756  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1005  MSE:  0.24111112933193524  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1006  MSE:  0.2403839568938664  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1007  MSE:  0.23966436877418673  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1008  MSE:  0.2389522528676621  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1009  MSE:  0.2382474853269941  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1010  MSE:  0.23754993161240998  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1011  MSE:  0.23685944748606366  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1012  MSE:  0.2361758799514405  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1013  MSE:  0.23549906813849414  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1014  MSE:  0.23482884413563127  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1015  MSE:  0.23416503376997166  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1016  MSE:  0.23350745733747966  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1017  MSE:  0.23285593028474505  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1018  MSE:  0.23221026384419374  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1019  MSE:  0.23157026562462485  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1020  MSE:  0.23093574015890259  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1021  MSE:  0.23030648941066828  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1022  MSE:  0.2296823132418776  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1023  MSE:  0.22906300984295716  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1024  MSE:  0.22844837612730595  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1025  MSE:  0.2278382080918638  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1026  MSE:  0.22723230114539508  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1027  MSE:  0.22663045040612828  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1028  MSE:  0.22603245097033867  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1029  MSE:  0.22543809815344318  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1030  MSE:  0.22484718770513099  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1031  MSE:  0.22425951600005198  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1032  MSE:  0.22367488020553186  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1033  MSE:  0.2230930784277743  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1034  MSE:  0.2225139098379806  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1035  MSE:  0.2219371747797932  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1036  MSE:  0.22136267485944724  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1037  MSE:  0.22079021301997662  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1038  MSE:  0.22021959360081425  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1039  MSE:  0.2196506223840801  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1040  MSE:  0.2190831066288254  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1041  MSE:  0.21851685509447463  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1042  MSE:  0.21795167805467808  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1043  MSE:  0.21738738730273355  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1044  MSE:  0.2168237961497191  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1045  MSE:  0.21626071941643768  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1046  MSE:  0.21569797342022237  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1047  MSE:  0.21513537595761884  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1048  MSE:  0.21457274628391337  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1049  MSE:  0.2140099050904289  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1050  MSE:  0.21344667448046611  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1051  MSE:  0.21288287794469882  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1052  MSE:  0.21231834033680647  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1053  MSE:  0.2117528878500467  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1054  MSE:  0.21118634799542055  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1055  MSE:  0.21061854958202653  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1056  MSE:  0.21004932270013005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1057  MSE:  0.20947849870741495  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1058  MSE:  0.20890591021880775  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1059  MSE:  0.20833139110021026  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1060  MSE:  0.2077547764663916  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1061  MSE:  0.20717590268323047  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1062  MSE:  0.20659460737441337  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1063  MSE:  0.2060107294326417  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1064  MSE:  0.20542410903530817  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1065  MSE:  0.20483458766453794  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1066  MSE:  0.20424200813143556  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1067  MSE:  0.20364621460428614  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1068  MSE:  0.20304705264041825  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1069  MSE:  0.2024443692213513  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1070  MSE:  0.20183801279082236  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1071  MSE:  0.20122783329520288  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1072  MSE:  0.20061368222578646  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1073  MSE:  0.19999541266237733  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1074  MSE:  0.1993728793175579  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1075  MSE:  0.19874593858097828  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1076  MSE:  0.19811444856295815  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1077  MSE:  0.19747826913664687  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1078  MSE:  0.19683726197791254  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1079  MSE:  0.1961912906020709  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1080  MSE:  0.19554022039645577  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1081  MSE:  0.19488391864770654  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1082  MSE:  0.19422225456247455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1083  MSE:  0.1935550992800428  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1084  MSE:  0.19288232587506007  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1085  MSE:  0.19220380934823228  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1086  MSE:  0.19151942660238447  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1087  MSE:  0.1908290564007265  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1088  MSE:  0.19013257930350838  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1089  MSE:  0.18942987757845037  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1090  MSE:  0.18872083507937615  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1091  MSE:  0.18800533708641634  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1092  MSE:  0.18728327009987158  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1093  MSE:  0.18655452157848942  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1094  MSE:  0.18581897961134494  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1095  MSE:  0.1850765325109025  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1096  MSE:  0.1843270683131367  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1097  MSE:  0.18357047416884478  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1098  MSE:  0.18280663560858432  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1099  MSE:  0.18203543566212047  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1100  MSE:  0.1812567538119319  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1101  MSE:  0.1804704647593238  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1102  MSE:  0.1796764369813231  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1103  MSE:  0.17887453105669593  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1104  MSE:  0.17806459774066685  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1105  MSE:  0.17724647577015437  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1106  MSE:  0.17641998938502923  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1107  MSE:  0.17558494555619858  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1108  MSE:  0.17474113091846868  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1109  MSE:  0.17388830841554392  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1110  MSE:  0.17302621367613785  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1111  MSE:  0.17215455115448858  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1112  MSE:  0.17127299008551158  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1113  MSE:  0.17038116032441997  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1114  MSE:  0.16947864816294214  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1115  MSE:  0.16856499223881574  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1116  MSE:  0.1676396796817409  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1117  MSE:  0.16670214266665362  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1118  MSE:  0.16575175557311758  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1119  MSE:  0.16478783297663013  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1120  MSE:  0.16380962872216054  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1121  MSE:  0.16281633635044504  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1122  MSE:  0.16180709116151595  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1123  MSE:  0.1607809742052647  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1124  MSE:  0.1597370184832765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1125  MSE:  0.15867421762729458  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1126  MSE:  0.15759153728535455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1127  MSE:  0.15648792939478565  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1128  MSE:  0.1553623494506841  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1129  MSE:  0.15421377678807432  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1130  MSE:  0.15304123778593726  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1131  MSE:  0.15184383177238164  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1132  MSE:  0.15062075926453763  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1133  MSE:  0.14937135201726573  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1134  MSE:  0.14809510418593858  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1135  MSE:  0.14679170373608283  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1136  MSE:  0.14546106306374826  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1137  MSE:  0.14410334763400626  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1138  MSE:  0.1427190013116001  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1139  MSE:  0.1413087669600194  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1140  MSE:  0.1398737008374977  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1141  MSE:  0.13841517933616382  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1142  MSE:  0.1369348967100455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1143  MSE:  0.13543485263351807  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1144  MSE:  0.13391732873567583  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1145  MSE:  0.13238485367293323  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1146  MSE:  0.13084015682767944  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1147  MSE:  0.1292861113380283  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1148  MSE:  0.12772566784165737  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1149  MSE:  0.12616178100997402  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1150  MSE:  0.12459733160027668  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1151  MSE:  0.1230350472996864  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1152  MSE:  0.12147742601368776  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1153  MSE:  0.11992666541283815  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1154  MSE:  0.11838460246210275  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1155  MSE:  0.11685266631172743  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1156  MSE:  0.11533184734685645  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1157  MSE:  0.11382268441791667  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1158  MSE:  0.11232527136303849  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1159  MSE:  0.11083928295020558  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1160  MSE:  0.10936401937099438  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1161  MSE:  0.10789846746238031  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1162  MSE:  0.10644137596460129  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1163  MSE:  0.10499134138231642  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1164  MSE:  0.1035469004433308  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1165  MSE:  0.10210662478163904  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1166  MSE:  0.10066921334203394  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1167  MSE:  0.09923357813346274  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1168  MSE:  0.09779891935051734  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1169  MSE:  0.09636478651712961  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1170  MSE:  0.09493112314049004  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1171  MSE:  0.09349829333285524  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1172  MSE:  0.0920670898889206  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1173  MSE:  0.09063872431694799  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1174  MSE:  0.08921480023976983  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1175  MSE:  0.08779727234643438  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1176  MSE:  0.08638839364499697  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1177  MSE:  0.08499065412040462  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1178  MSE:  0.08360671403715728  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1179  MSE:  0.08223933506116253  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1180  MSE:  0.08089131213820049  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1181  MSE:  0.07956540869644163  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1182  MSE:  0.07826429728025788  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1183  MSE:  0.07699050721427876  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1184  MSE:  0.0757463803793555  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1185  MSE:  0.07453403568846992  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1186  MSE:  0.0733553424059639  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1187  MSE:  0.07221190207474064  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1188  MSE:  0.07110503851305186  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1189  MSE:  0.07003579511761397  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1190  MSE:  0.0690049385599208  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1191  MSE:  0.06801296788127678  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1192  MSE:  0.06706012796880745  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1193  MSE:  0.0661464264190486  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1194  MSE:  0.06527165285613105  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1195  MSE:  0.06443539985698553  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1196  MSE:  0.06363708473749978  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1197  MSE:  0.06287597156209873  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1198  MSE:  0.06215119284904806  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1199  MSE:  0.06146177054932636  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1200  MSE:  0.06080663597495973  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1201  MSE:  0.060184648440943934  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1202  MSE:  0.059594612462001754  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1203  MSE:  0.05903529341121969  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1204  MSE:  0.058505431602031636  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1205  MSE:  0.05800375479919469  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1206  MSE:  0.05752898919870455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1207  MSE:  0.05707986894272892  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1208  MSE:  0.05665514425412345  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1209  MSE:  0.056253588287725954  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1210  MSE:  0.055874002802884834  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1211  MSE:  0.05551522276496541  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1212  MSE:  0.05517611998358037  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1213  MSE:  0.05485560589278033  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1214  MSE:  0.05455263357415625  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1215  MSE:  0.05426619911817751  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1216  MSE:  0.053995342412619975  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1217  MSE:  0.05373914744000128  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1218  MSE:  0.05349674215880868  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1219  MSE:  0.05326729803608904  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1220  MSE:  0.05305002929207337  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1221  MSE:  0.05284419191077092  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1222  MSE:  0.0526490824641553  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1223  MSE:  0.05246403679172714  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1224  MSE:  0.052288428571733714  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1225  MSE:  0.05212166781539695  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1226  MSE:  0.05196319931100401  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1227  MSE:  0.051812501040615765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1228  MSE:  0.05166908258855207  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1229  MSE:  0.051532483557597306  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1230  MSE:  0.05140227200600161  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1231  MSE:  0.05127804291588305  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1232  MSE:  0.05115941670144822  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1233  MSE:  0.05104603776359757  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1234  MSE:  0.0509375730958189  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1235  MSE:  0.05083371094493371  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1236  MSE:  0.05073415952904341  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1237  MSE:  0.050638645814089285  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1238  MSE:  0.05054691434956341  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1239  MSE:  0.05045872616325858  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1240  MSE:  0.05037385771438541  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1241  MSE:  0.050292099903930194  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1242  MSE:  0.050213257140783556  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1243  MSE:  0.05013714646190428  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1244  MSE:  0.050063596704548026  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1245  MSE:  0.049992447728509005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1246  MSE:  0.04992354968612966  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1247  MSE:  0.049856762337901525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1248  MSE:  0.04979195441132403  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1249  MSE:  0.04972900300082827  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1250  MSE:  0.04966779300647984  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1251  MSE:  0.04960821660931388  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1252  MSE:  0.049550172781159016  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1253  MSE:  0.049493566826909835  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1254  MSE:  0.04943830995725592  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1255  MSE:  0.049384318889994415  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1256  MSE:  0.04933151547811229  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1257  MSE:  0.049279826362920505  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1258  MSE:  0.04922918265062316  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1259  MSE:  0.049179519610775276  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1260  MSE:  0.049130776395190645  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1261  MSE:  0.04908289577592241  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1262  MSE:  0.04903582390103851  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1263  MSE:  0.04898951006700303  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1264  MSE:  0.04894390650651573  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1265  MSE:  0.04889896819077213  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1266  MSE:  0.0488546526451491  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1267  MSE:  0.04881091977741048  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1268  MSE:  0.048767731717560374  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1269  MSE:  0.048725052668557284  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1270  MSE:  0.048682848767139256  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1271  MSE:  0.048641087954070746  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1272  MSE:  0.04859973985316247  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1273  MSE:  0.04855877565846848  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1274  MSE:  0.048518168029110506  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1275  MSE:  0.048477890991186594  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1276  MSE:  0.048437919846318914  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1277  MSE:  0.04839823108635862  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1278  MSE:  0.048358802313853  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1279  MSE:  0.04831961216788523  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1280  MSE:  0.04828064025491523  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1281  MSE:  0.048241867084304155  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1282  MSE:  0.0482032740081991  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1283  MSE:  0.048164843165495956  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1284  MSE:  0.04812655742960645  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1285  MSE:  0.048088400359787835  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1286  MSE:  0.048050356155789314  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1287  MSE:  0.04801240961561413  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1288  MSE:  0.04797454609617678  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1289  MSE:  0.047936751476689035  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1290  MSE:  0.047899012124580816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1291  MSE:  0.047861314863799055  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1292  MSE:  0.047823646945336785  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1293  MSE:  0.04778599601984398  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1294  MSE:  0.04774835011219005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1295  MSE:  0.0477106975978465  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1296  MSE:  0.0476730271809929  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1297  MSE:  0.04763532787420677  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1298  MSE:  0.04759758897967262  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1299  MSE:  0.047559800071779304  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1300  MSE:  0.047521950981043615  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1301  MSE:  0.04748403177925793  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1302  MSE:  0.047446032765799416  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1303  MSE:  0.047407944455009066  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1304  MSE:  0.04736975756459047  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1305  MSE:  0.04733146300494514  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1306  MSE:  0.04729305186939779  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1307  MSE:  0.047254515425243705  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1308  MSE:  0.047215845105567394  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1309  MSE:  0.04717703250178695  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1310  MSE:  0.04713806935685854  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1311  MSE:  0.04709894755911724  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1312  MSE:  0.04705965913669385  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1313  MSE:  0.04702019625247465  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1314  MSE:  0.046980551199562265  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1315  MSE:  0.04694071639719966  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1316  MSE:  0.046900684387128355  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1317  MSE:  0.04686044783033902  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1318  MSE:  0.04681999950418643  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1319  MSE:  0.04677933229984259  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1320  MSE:  0.0467384392200522  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1321  MSE:  0.04669731337716565  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1322  MSE:  0.04665594799142699  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1323  MSE:  0.04661433638948717  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1324  MSE:  0.04657247200312387  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1325  MSE:  0.04653034836813388  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1326  MSE:  0.04648795912339119  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1327  MSE:  0.04644529801004551  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1328  MSE:  0.046402358870834146  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1329  MSE:  0.04635913564949002  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1330  MSE:  0.046315622390245226  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1331  MSE:  0.046271813237386696  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1332  MSE:  0.046227702434863606  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1333  MSE:  0.04618328432593221  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1334  MSE:  0.0461385533528158  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1335  MSE:  0.04609350405636913  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1336  MSE:  0.046048131075736534  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1337  MSE:  0.04600242914798886  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1338  MSE:  0.04595639310773104  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1339  MSE:  0.04591001788665962  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1340  MSE:  0.045863298513080734  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1341  MSE:  0.04581623011134969  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1342  MSE:  0.04576880790125978  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1343  MSE:  0.04572102719733596  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1344  MSE:  0.04567288340805513  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1345  MSE:  0.045624372034970134  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1346  MSE:  0.04557548867174121  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1347  MSE:  0.04552622900306604  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1348  MSE:  0.04547658880350256  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1349  MSE:  0.045426563936186326  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1350  MSE:  0.04537615035144078  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1351  MSE:  0.04532534408527113  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1352  MSE:  0.04527414125774765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1353  MSE:  0.04522253807127951  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1354  MSE:  0.04517053080877681  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1355  MSE:  0.04511811583169687  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1356  MSE:  0.04506528957798866  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1357  MSE:  0.04501204855993694  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1358  MSE:  0.04495838936189358  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1359  MSE:  0.04490430863791883  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1360  MSE:  0.04484980310933636  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1361  MSE:  0.04479486956219606  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1362  MSE:  0.04473950484465676  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1363  MSE:  0.044683705864306825  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1364  MSE:  0.04462746958540904  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1365  MSE:  0.04457079302610085  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1366  MSE:  0.04451367325554092  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1367  MSE:  0.04445610739102357  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1368  MSE:  0.044398092595066704  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1369  MSE:  0.04433962607248244  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1370  MSE:  0.04428070506744281  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1371  MSE:  0.044221326860558276  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1372  MSE:  0.04416148876597152  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1373  MSE:  0.04410118812848594  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1374  MSE:  0.044040422320734594  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1375  MSE:  0.04397918874041479  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1376  MSE:  0.043917484807596434  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1377  MSE:  0.04385530796210089  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1378  MSE:  0.04379265566099997  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1379  MSE:  0.04372952537621687  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1380  MSE:  0.04366591459225931  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1381  MSE:  0.04360182080409382  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1382  MSE:  0.0435372415151806  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1383  MSE:  0.04347217423567719  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1384  MSE:  0.04340661648082961  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1385  MSE:  0.043340565769558075  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1386  MSE:  0.0432740196232703  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1387  MSE:  0.0432069755648856  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1388  MSE:  0.043139431118105635  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1389  MSE:  0.0430713838069507  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1390  MSE:  0.043002831155542195  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1391  MSE:  0.04293377068818184  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1392  MSE:  0.04286419992970514  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1393  MSE:  0.042794116406149406  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1394  MSE:  0.04272351764572708  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1395  MSE:  0.04265240118011717  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1396  MSE:  0.04258076454609048  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1397  MSE:  0.04250860528747168  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1398  MSE:  0.04243592095744525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1399  MSE:  0.04236270912121705  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1400  MSE:  0.042288967359019755  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1401  MSE:  0.04221469326949605  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1402  MSE:  0.04213988447343089  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1403  MSE:  0.042064538617858784  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1404  MSE:  0.04198865338052885  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1405  MSE:  0.04191222647474673  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1406  MSE:  0.04183525565456825  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1407  MSE:  0.041757738720366235  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1408  MSE:  0.041679673524745914  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1409  MSE:  0.04160105797881509  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1410  MSE:  0.041521890058799575  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1411  MSE:  0.041442167812990115  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1412  MSE:  0.04136188936902415  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1413  MSE:  0.041281052941468166  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1414  MSE:  0.041199656839715666  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1415  MSE:  0.0411176994761643  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1416  MSE:  0.0410351793746639  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1417  MSE:  0.040952095179223394  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1418  MSE:  0.04086844566294718  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1419  MSE:  0.040784229737189696  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1420  MSE:  0.040699446460897896  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1421  MSE:  0.040614095050126434  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1422  MSE:  0.04052817488769658  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1423  MSE:  0.04044168553296503  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1424  MSE:  0.04035462673169525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1425  MSE:  0.04026699842597397  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1426  MSE:  0.04017880076416765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1427  MSE:  0.04009003411087393  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1428  MSE:  0.040000699056830964  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1429  MSE:  0.03991079642876928  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1430  MSE:  0.03982032729914414  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1431  MSE:  0.039729292995735924  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1432  MSE:  0.039637695111067316  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1433  MSE:  0.039545535511605494  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1434  MSE:  0.03945281634670372  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1435  MSE:  0.039359540057252514  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1436  MSE:  0.039265709383996164  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1437  MSE:  0.039171327375468294  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1438  MSE:  0.03907639739551784  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1439  MSE:  0.038980923130380256  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1440  MSE:  0.03888490859524626  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1441  MSE:  0.03878835814030766  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1442  MSE:  0.03869127645621796  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1443  MSE:  0.03859366857895005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1444  MSE:  0.03849553989400178  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1445  MSE:  0.03839689613991696  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1446  MSE:  0.03829774341108789  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1447  MSE:  0.03819808815980411  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1448  MSE:  0.03809793719752252  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1449  MSE:  0.03799729769531968  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1450  MSE:  0.037896177183507976  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1451  MSE:  0.03779458355038768  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1452  MSE:  0.03769252504011053  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1453  MSE:  0.03759001024964784  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1454  MSE:  0.037487048124825  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1455  MSE:  0.03738364795543416  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1456  MSE:  0.03727981936939712  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1457  MSE:  0.0371755723259863  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1458  MSE:  0.03707091710808378  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1459  MSE:  0.036965864313507533  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1460  MSE:  0.036860424845376  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1461  MSE:  0.03675460990155178  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1462  MSE:  0.0366484309631548  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1463  MSE:  0.03654189978217604  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1464  MSE:  0.036435028368205294  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1465  MSE:  0.03632782897430045  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1466  MSE:  0.036220314082032176  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1467  MSE:  0.03611249638572303  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1468  MSE:  0.036004388775939075  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1469  MSE:  0.035896004322253254  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1470  MSE:  0.035787356255332865  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1471  MSE:  0.03567845794840447  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1472  MSE:  0.03556932289813854  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1473  MSE:  0.035459964705002685  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1474  MSE:  0.03535039705315383  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1475  MSE:  0.035240633689907416  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1476  MSE:  0.03513068840486891  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1477  MSE:  0.03502057500876517  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1478  MSE:  0.0349103073120437  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1479  MSE:  0.03479989910332364  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1480  MSE:  0.03468936412774048  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1481  MSE:  0.03457871606525574  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1482  MSE:  0.034467968509008386  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1483  MSE:  0.03435713494376185  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1484  MSE:  0.034246228724504196  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1485  MSE:  0.03413526305530052  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1486  MSE:  0.034024250968411475  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1487  MSE:  0.03391320530377518  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1488  MSE:  0.033802138688895575  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1489  MSE:  0.03369106351921156  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1490  MSE:  0.03357999193896321  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1491  MSE:  0.033468935822657454  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1492  MSE:  0.03335790675715428  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1493  MSE:  0.033246916024403425  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1494  MSE:  0.03313597458491939  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1495  MSE:  0.033025093062000736  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1496  MSE:  0.032914281726728764  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1497  MSE:  0.032803550483803005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1498  MSE:  0.0326929088582195  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1499  MSE:  0.03258236598282655  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1500  MSE:  0.032471930586780634  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1501  MSE:  0.032361610984918236  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1502  MSE:  0.032251415068059286  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1503  MSE:  0.03214135029425261  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1504  MSE:  0.032031423680983016  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1505  MSE:  0.031921641798316525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1506  MSE:  0.03181201076303514  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1507  MSE:  0.03170253623370041  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1508  MSE:  0.0315932234067072  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1509  MSE:  0.03148407701327522  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1510  MSE:  0.031375101317396986  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1511  MSE:  0.03126630011472482  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1512  MSE:  0.031157676732415478  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1513  MSE:  0.031049234029877724  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1514  MSE:  0.03094097440045863  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1515  MSE:  0.03083289977403905  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1516  MSE:  0.030725011620542114  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1517  MSE:  0.030617310954327993  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1518  MSE:  0.030509798339502958  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1519  MSE:  0.03040247389610266  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1520  MSE:  0.03029533730717983  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1521  MSE:  0.03018838782676975  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1522  MSE:  0.03008162428875616  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1523  MSE:  0.029975045116622998  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1524  MSE:  0.029868648334115733  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1525  MSE:  0.029762431576795964  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1526  MSE:  0.029656392104534555  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1527  MSE:  0.029550526814904422  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1528  MSE:  0.029444832257523968  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1529  MSE:  0.029339304649347375  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1530  MSE:  0.02923393989090035  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1531  MSE:  0.02912873358348056  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1532  MSE:  0.02902368104733909  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1533  MSE:  0.028918777340804704  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1534  MSE:  0.028814017280395364  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1535  MSE:  0.028709395461875876  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1536  MSE:  0.02860490628225636  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1537  MSE:  0.028500543962699847  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1538  MSE:  0.02839630257231232  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1539  MSE:  0.028292176052773343  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1540  MSE:  0.02818815824372559  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1541  MSE:  0.028084242908893487  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1542  MSE:  0.02798042376281286  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1543  MSE:  0.02787669449810282  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1544  MSE:  0.027773048813151333  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1545  MSE:  0.027669480440119035  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1546  MSE:  0.02756598317309281  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1547  MSE:  0.027462550896279178  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1548  MSE:  0.02735917761206571  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1549  MSE:  0.02725585746879419  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1550  MSE:  0.02715258478806065  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1551  MSE:  0.02704935409138472  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1552  MSE:  0.026946160126091866  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1553  MSE:  0.026842997890177394  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1554  MSE:  0.026739862656036006  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1555  MSE:  0.026636749992875196  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1556  MSE:  0.026533655787644318  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1557  MSE:  0.02643057626435463  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1558  MSE:  0.02632750800165752  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1559  MSE:  0.02622444794855896  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1560  MSE:  0.026121393438195847  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1561  MSE:  0.02601834219957868  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1562  MSE:  0.025915292367271595  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1563  MSE:  0.02581224248897103  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1564  MSE:  0.025709191530967554  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1565  MSE:  0.02560613888152312  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1566  MSE:  0.025503084352188475  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1567  MSE:  0.02540002817712859  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1568  MSE:  0.02529697101052689  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1569  MSE:  0.025193913922159163  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1570  MSE:  0.02509085839126099  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1571  MSE:  0.024987806298791514  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1572  MSE:  0.02488475991825608  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1573  MSE:  0.024781721905197136  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1574  MSE:  0.024678695285522093  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1575  MSE:  0.02457568344282683  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1576  MSE:  0.024472690104828592  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1577  MSE:  0.024369719329107007  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1578  MSE:  0.024266775488275515  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1579  MSE:  0.024163863254711152  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1580  MSE:  0.02406098758500472  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1581  MSE:  0.023958153704249117  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1582  MSE:  0.023855367090275737  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1583  MSE:  0.02375263345793998  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1584  MSE:  0.023649958743578364  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1585  MSE:  0.023547349089697824  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1586  MSE:  0.023444810829969964  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1587  MSE:  0.023342350474613034  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1588  MSE:  0.023239974696197555  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1589  MSE:  0.023137690315918582  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1590  MSE:  0.023035504290372995  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1591  MSE:  0.022933423698843998  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1592  MSE:  0.02283145573115894  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1593  MSE:  0.022729607676047182  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1594  MSE:  0.022627886910081985  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1595  MSE:  0.022526300887124047  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1596  MSE:  0.02242485712833295  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1597  MSE:  0.02232356321265191  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1598  MSE:  0.022222426767820262  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1599  MSE:  0.022121455461836474  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1600  MSE:  0.022020656994884053  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1601  MSE:  0.02192003909168119  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1602  MSE:  0.021819609494225456  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1603  MSE:  0.021719375954919802  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1604  MSE:  0.02161934623004617  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1605  MSE:  0.021519528073558024  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1606  MSE:  0.0214199292311857  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1607  MSE:  0.021320557434792175  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1608  MSE:  0.021221420397017375  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1609  MSE:  0.021122525806118293  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1610  MSE:  0.02102388132105343  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1611  MSE:  0.020925494566738277  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1612  MSE:  0.02082737312949209  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1613  MSE:  0.020729524552646823  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1614  MSE:  0.020631956332299715  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1615  MSE:  0.02053467591320405  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1616  MSE:  0.020437690684803046  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1617  MSE:  0.02034100797734998  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1618  MSE:  0.020244635058173723  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1619  MSE:  0.020148579128021104  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1620  MSE:  0.020052847317515696  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1621  MSE:  0.019957446683691798  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1622  MSE:  0.01986238420663499  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1623  MSE:  0.019767666786204955  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1624  MSE:  0.019673301238831944  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1625  MSE:  0.019579294294415826  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1626  MSE:  0.019485652593295398  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1627  MSE:  0.01939238268330119  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1628  MSE:  0.019299491016897798  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1629  MSE:  0.019206983948397257  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1630  MSE:  0.019114867731280776  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1631  MSE:  0.019023148515573025  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1632  MSE:  0.018931832345328338  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1633  MSE:  0.018840925156193372  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1634  MSE:  0.018750432773064707  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1635  MSE:  0.0186603609078263  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1636  MSE:  0.018570715157196094  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1637  MSE:  0.01848150100065076  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1638  MSE:  0.018392723798456093  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1639  MSE:  0.018304388789781606  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1640  MSE:  0.018216501090930187  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1641  MSE:  0.01812906569364389  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1642  MSE:  0.01804208746353743  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1643  MSE:  0.01795557113860653  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1644  MSE:  0.01786952132784451  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1645  MSE:  0.017783942509975306  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1646  MSE:  0.017698839032274347  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1647  MSE:  0.017614215109485704  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1648  MSE:  0.01753007482286471  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1649  MSE:  0.017446422119300125  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1650  MSE:  0.01736326081055861  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1651  MSE:  0.01728059457261336  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1652  MSE:  0.017198426945083493  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1653  MSE:  0.017116761330779492  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1654  MSE:  0.01703560099533582  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1655  MSE:  0.016954949066956205  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1656  MSE:  0.016874808536244526  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1657  MSE:  0.016795182256144296  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1658  MSE:  0.01671607294196617  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1659  MSE:  0.016637483171510977  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1660  MSE:  0.01655941538528585  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1661  MSE:  0.016481871886818832  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1662  MSE:  0.016404854843046168  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1663  MSE:  0.01632836628480068  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1664  MSE:  0.01625240810738264  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1665  MSE:  0.01617698207121287  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1666  MSE:  0.016102089802565393  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1667  MSE:  0.016027732794380665  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1668  MSE:  0.015953912407159317  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1669  MSE:  0.015880629869923085  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1670  MSE:  0.015807886281256376  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1671  MSE:  0.01573568261041173  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1672  MSE:  0.015664019698485344  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1673  MSE:  0.015592898259658122  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1674  MSE:  0.015522318882499093  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1675  MSE:  0.015452282031327898  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1676  MSE:  0.015382788047634181  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1677  MSE:  0.015313837151560117  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1678  MSE:  0.015245429443422488  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1679  MSE:  0.015177564905298325  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1680  MSE:  0.015110243402645284  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1681  MSE:  0.015043464685982647  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1682  MSE:  0.014977228392599715  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1683  MSE:  0.014911534048315928  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1684  MSE:  0.014846381069268938  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1685  MSE:  0.014781768763752347  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1686  MSE:  0.014717696334073332  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1687  MSE:  0.01465416287843745  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1688  MSE:  0.014591167392886358  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1689  MSE:  0.01452870877323256  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1690  MSE:  0.01446678581703253  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1691  MSE:  0.014405397225584059  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1692  MSE:  0.014344541605934508  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1693  MSE:  0.01428421747290747  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1694  MSE:  0.01422442325115191  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1695  MSE:  0.014165157277199546  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1696  MSE:  0.014106417801524138  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1697  MSE:  0.014048202990634681  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1698  MSE:  0.013990510929143074  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1699  MSE:  0.013933339621863565  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1700  MSE:  0.01387668699590181  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1701  MSE:  0.013820550902756594  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1702  MSE:  0.013764929120401375  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1703  MSE:  0.013709819355388665  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1704  MSE:  0.01365521924493177  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1705  MSE:  0.01360112635899845  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1706  MSE:  0.013547538202375798  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1707  MSE:  0.01349445221675263  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1708  MSE:  0.013441865782775316  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1709  MSE:  0.013389776222095655  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1710  MSE:  0.01333818079941174  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1711  MSE:  0.013287076724497958  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1712  MSE:  0.013236461154199535  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1713  MSE:  0.013186331194450738  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1714  MSE:  0.013136683902232766  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1715  MSE:  0.013087516287546551  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1716  MSE:  0.013038825315348388  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1717  MSE:  0.012990607907480417  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1718  MSE:  0.012942860944557854  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1719  MSE:  0.01289558126787187  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1720  MSE:  0.012848765681226395  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1721  MSE:  0.012802410952786903  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1722  MSE:  0.012756513816886235  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1723  MSE:  0.012711070975817803  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1724  MSE:  0.012666079101593408  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1725  MSE:  0.012621534837684125  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1726  MSE:  0.012577434800730274  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1727  MSE:  0.01253377558222491  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1728  MSE:  0.01249055375017673  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1729  MSE:  0.01244776585073731  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1730  MSE:  0.012405408409804317  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1731  MSE:  0.012363477934598077  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1732  MSE:  0.012321970915207299  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1733  MSE:  0.012280883826106235  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1734  MSE:  0.012240213127647913  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1735  MSE:  0.012199955267518358  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1736  MSE:  0.012160106682173993  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1737  MSE:  0.012120663798237653  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1738  MSE:  0.012081623033877513  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1739  MSE:  0.01204298080014159  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1740  MSE:  0.01200473350227862  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1741  MSE:  0.01196687754101695  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1742  MSE:  0.011929409313820968  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1743  MSE:  0.011892325216113683  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1744  MSE:  0.011855621642472934  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1745  MSE:  0.011819294987798075  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1746  MSE:  0.011783341648444164  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1747  MSE:  0.011747758023328976  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1748  MSE:  0.011712540515013697  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1749  MSE:  0.011677685530749313  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1750  MSE:  0.011643189483496066  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1751  MSE:  0.011609048792921888  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1752  MSE:  0.011575259886354168  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1753  MSE:  0.011541819199726123  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1754  MSE:  0.011508723178481905  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1755  MSE:  0.011475968278451902  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1756  MSE:  0.011443550966707353  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1757  MSE:  0.011411467722389736  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1758  MSE:  0.011379715037506639  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1759  MSE:  0.011348289417702648  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1760  MSE:  0.011317187383006508  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1761  MSE:  0.011286405468554615  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1762  MSE:  0.011255940225283933  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1763  MSE:  0.011225788220597674  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1764  MSE:  0.011195946039018603  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1765  MSE:  0.011166410282802387  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1766  MSE:  0.011137177572539368  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1767  MSE:  0.01110824454772312  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1768  MSE:  0.011079607867303505  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1769  MSE:  0.011051264210211641  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1770  MSE:  0.011023210275863985  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1771  MSE:  0.010995442784646108  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1772  MSE:  0.010967958478368036  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1773  MSE:  0.010940754120711129  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1774  MSE:  0.010913826497637101  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1775  MSE:  0.010887172417790615  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1776  MSE:  0.010860788712876863  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1777  MSE:  0.010834672238014533  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1778  MSE:  0.010808819872078643  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1779  MSE:  0.010783228518014457  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1780  MSE:  0.010757895103142234  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1781  MSE:  0.010732816579437185  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1782  MSE:  0.01070798992379213  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1783  MSE:  0.010683412138269573  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1784  MSE:  0.010659080250326435  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1785  MSE:  0.010634991313029557  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1786  MSE:  0.010611142405254329  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1787  MSE:  0.010587530631860928  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1788  MSE:  0.010564153123868747  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1789  MSE:  0.010541007038599432  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1790  MSE:  0.010518089559815445  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1791  MSE:  0.010495397897843577  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1792  MSE:  0.010472929289681257  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1793  MSE:  0.010450680999093126  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1794  MSE:  0.010428650316687525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1795  MSE:  0.01040683455999159  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1796  MSE:  0.010385231073504133  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1797  MSE:  0.01036383722873618  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1798  MSE:  0.010342650424250005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1799  MSE:  0.010321668085674682  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1800  MSE:  0.010300887665716714  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1801  MSE:  0.010280306644162246  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1802  MSE:  0.010259922527861929  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1803  MSE:  0.010239732850711412  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1804  MSE:  0.010219735173621031  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1805  MSE:  0.010199927084470976  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1806  MSE:  0.010180306198068397  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1807  MSE:  0.010160870156084665  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1808  MSE:  0.010141616626985844  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1809  MSE:  0.010122543305965388  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1810  MSE:  0.010103647914852108  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1811  MSE:  0.010084928202029926  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1812  MSE:  0.010066381942329415  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1813  MSE:  0.010048006936926503  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1814  MSE:  0.01002980101323571  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1815  MSE:  0.010011762024784581  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1816  MSE:  0.00999388785109381  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1817  MSE:  0.009976176397541569  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1818  MSE:  0.009958625595236801  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1819  MSE:  0.009941233400862603  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1820  MSE:  0.009923997796545129  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1821  MSE:  0.00990691678968954  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1822  MSE:  0.009889988412827594  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1823  MSE:  0.009873210723455523  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1824  MSE:  0.009856581803869736  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1825  MSE:  0.009840099760992406  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1826  MSE:  0.009823762726202934  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1827  MSE:  0.00980756885515422  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1828  MSE:  0.00979151632759647  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1829  MSE:  0.009775603347188801  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1830  MSE:  0.009759828141314294  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1831  MSE:  0.00974418896088651  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1832  MSE:  0.009728684080156827  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1833  MSE:  0.00971331179651877  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1834  MSE:  0.009698070430309824  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1835  MSE:  0.009682958324605752  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1836  MSE:  0.00966797384502505  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1837  MSE:  0.00965311537951755  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1838  MSE:  0.009638381338158919  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1839  MSE:  0.00962377015294417  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1840  MSE:  0.00960928027757592  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1841  MSE:  0.00959491018725292  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1842  MSE:  0.009580658378456463  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1843  MSE:  0.009566523368736668  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1844  MSE:  0.009552503696499078  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1845  MSE:  0.00953859792078904  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1846  MSE:  0.009524804621071541  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1847  MSE:  0.009511122397017452  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1848  MSE:  0.00949754986828489  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1849  MSE:  0.009484085674300997  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1850  MSE:  0.009470728474042142  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1851  MSE:  0.00945747694581816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1852  MSE:  0.009444329787049953  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1853  MSE:  0.009431285714055753  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1854  MSE:  0.00941834346182573  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1855  MSE:  0.009405501783808327  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1856  MSE:  0.009392759451690762  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1857  MSE:  0.00938011525518046  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1858  MSE:  0.009367568001788099  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1859  MSE:  0.00935511651660891  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1860  MSE:  0.009342759642108556  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1861  MSE:  0.00933049623790461  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1862  MSE:  0.009318325180554475  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1863  MSE:  0.009306245363337535  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1864  MSE:  0.00929425569604291  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1865  MSE:  0.00928235510475706  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1866  MSE:  0.009270542531652094  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1867  MSE:  0.00925881693477243  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1868  MSE:  0.009247177287828917  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1869  MSE:  0.009235622579985654  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1870  MSE:  0.009224151815656263  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1871  MSE:  0.009212764014295024  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1872  MSE:  0.009201458210192168  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1873  MSE:  0.009190233452271742  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1874  MSE:  0.009179088803885037  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1875  MSE:  0.009168023342613107  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1876  MSE:  0.009157036160063586  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1877  MSE:  0.00914612636167573  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1878  MSE:  0.009135293066519865  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1879  MSE:  0.009124535407103741  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1880  MSE:  0.009113852529178562  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1881  MSE:  0.009103243591545164  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1882  MSE:  0.009092707765862545  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1883  MSE:  0.009082244236460361  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1884  MSE:  0.009071852200149945  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1885  MSE:  0.009061530866038378  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1886  MSE:  0.009051279455341016  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1887  MSE:  0.0090410972012035  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1888  MSE:  0.00903098334851786  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1889  MSE:  0.009020937153740553  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1890  MSE:  0.00901095788471784  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1891  MSE:  0.009001044820508666  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1892  MSE:  0.008991197251208765  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1893  MSE:  0.008981414477779467  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1894  MSE:  0.008971695811875778  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1895  MSE:  0.008962040575676051  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1896  MSE:  0.008952448101717173  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1897  MSE:  0.00894291773272416  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1898  MSE:  0.008933448821450947  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1899  MSE:  0.008924040730514967  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1900  MSE:  0.008914692832236433  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1901  MSE:  0.00890540450848183  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1902  MSE:  0.008896175150505163  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1903  MSE:  0.008887004158789954  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1904  MSE:  0.008877890942903637  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1905  MSE:  0.008868834921335379  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1906  MSE:  0.008859835521354796  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1907  MSE:  0.00885089217885869  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1908  MSE:  0.008842004338228075  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1909  MSE:  0.008833171452177956  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1910  MSE:  0.008824392981621788  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1911  MSE:  0.008815668395523188  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1912  MSE:  0.008806997170761409  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1913  MSE:  0.008798378791992125  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1914  MSE:  0.008789812751507836  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1915  MSE:  0.008781298549112596  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1916  MSE:  0.00877283569197809  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1917  MSE:  0.008764423694520608  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1918  MSE:  0.008756062078270737  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1919  MSE:  0.008747750371744148  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1920  MSE:  0.00873948811031563  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1921  MSE:  0.008731274836097853  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1922  MSE:  0.00872311009781581  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1923  MSE:  0.008714993450688203  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1924  MSE:  0.008706924456306978  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1925  MSE:  0.008698902682522954  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1926  MSE:  0.008690927703326124  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1927  MSE:  0.008682999098732366  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1928  MSE:  0.008675116454674083  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1929  MSE:  0.008667279362886818  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1930  MSE:  0.008659487420798756  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1931  MSE:  0.008651740231427668  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1932  MSE:  0.00864403740326927  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1933  MSE:  0.00863637855019708  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1934  MSE:  0.008628763291357517  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1935  MSE:  0.008621191251069138  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1936  MSE:  0.008613662058721229  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1937  MSE:  0.00860617534867965  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1938  MSE:  0.008598730760185313  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1939  MSE:  0.008591327937261943  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1940  MSE:  0.00858396652862198  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1941  MSE:  0.00857664618757294  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1942  MSE:  0.008569366571927431  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1943  MSE:  0.008562127343917722  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1944  MSE:  0.008554928170099384  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1945  MSE:  0.00854776872127305  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1946  MSE:  0.00854064867239702  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1947  MSE:  0.008533567702501256  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1948  MSE:  0.008526525494609994  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1949  MSE:  0.008519521735656316  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1950  MSE:  0.008512556116405945  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1951  MSE:  0.008505628331379068  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1952  MSE:  0.008498738078773433  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1953  MSE:  0.00849188506039054  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1954  MSE:  0.008485068981558699  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1955  MSE:  0.008478289551064548  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1956  MSE:  0.008471546481079516  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1957  MSE:  0.008464839487090883  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1958  MSE:  0.008458168287833816  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1959  MSE:  0.008451532605223583  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1960  MSE:  0.008444932164290415  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1961  MSE:  0.008438366693114846  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1962  MSE:  0.008431835922763255  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1963  MSE:  0.008425339587228716  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1964  MSE:  0.008418877423366857  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1965  MSE:  0.008412449170838778  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1966  MSE:  0.008406054572052157  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1967  MSE:  0.008399693372104632  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1968  MSE:  0.008393365318726285  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1969  MSE:  0.008387070162226626  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1970  MSE:  0.008380807655441377  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1971  MSE:  0.008374577553679308  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1972  MSE:  0.008368379614669363  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1973  MSE:  0.008362213598515678  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1974  MSE:  0.008356079267642714  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1975  MSE:  0.008349976386752344  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1976  MSE:  0.008343904722774914  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1977  MSE:  0.008337864044824207  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1978  MSE:  0.008331854124151392  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1979  MSE:  0.00832587473410521  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1980  MSE:  0.00831992565008643  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1981  MSE:  0.008314006649508652  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1982  MSE:  0.008308117511756018  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1983  MSE:  0.008302258018146371  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1984  MSE:  0.008296427951891493  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1985  MSE:  0.00829062709805997  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1986  MSE:  0.008284855243542524  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1987  MSE:  0.00827911217701458  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1988  MSE:  0.00827339768890304  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1989  MSE:  0.00826771157135415  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1990  MSE:  0.008262053618199424  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1991  MSE:  0.00825642362492327  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1992  MSE:  0.008250821388636483  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1993  MSE:  0.00824524670804298  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1994  MSE:  0.008239699383410988  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1995  MSE:  0.008234179216547833  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1996  MSE:  0.008228686010770604  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1997  MSE:  0.008223219570879839  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1998  MSE:  0.008217779703136262  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  1999  MSE:  0.008212366215235328  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2000  MSE:  0.008206978916279877  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2001  MSE:  0.008201617616763883  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2002  MSE:  0.008196282128543644  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2003  MSE:  0.00819097226482257  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2004  MSE:  0.008185687840122661  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2005  MSE:  0.008180428670273292  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2006  MSE:  0.008175194572384253  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2007  MSE:  0.00816998536483324  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2008  MSE:  0.008164800867243598  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2009  MSE:  0.008159640900467  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2010  MSE:  0.008154505286572444  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2011  MSE:  0.00814939384882347  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2012  MSE:  0.008144306411666375  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2013  MSE:  0.008139242800715043  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2014  MSE:  0.008134202842736504  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2015  MSE:  0.008129186365637002  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2016  MSE:  0.00812419319844984  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2017  MSE:  0.008119223171321908  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2018  MSE:  0.008114276115504257  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2019  MSE:  0.008109351863336205  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2020  MSE:  0.008104450248239314  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2021  MSE:  0.008099571104703704  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2022  MSE:  0.00809471426827924  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2023  MSE:  0.008089879575566747  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2024  MSE:  0.008085066864207352  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2025  MSE:  0.008080275972876007  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2026  MSE:  0.008075506741271334  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2027  MSE:  0.008070759010109802  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2028  MSE:  0.008066032621116796  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2029  MSE:  0.008061327417019622  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2030  MSE:  0.008056643241542077  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2031  MSE:  0.008051979939397888  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2032  MSE:  0.008047337356285005  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2033  MSE:  0.008042715338878003  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2034  MSE:  0.008038113734824846  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2035  MSE:  0.008033532392742779  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2036  MSE:  0.008028971162210127  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2037  MSE:  0.008024429893765336  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2038  MSE:  0.00801990843890029  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2039  MSE:  0.008015406650058426  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2040  MSE:  0.008010924380629737  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2041  MSE:  0.00800646148494808  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2042  MSE:  0.008002017818286493  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2043  MSE:  0.007997593236856204  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2044  MSE:  0.007993187597802702  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2045  MSE:  0.007988800759201508  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2046  MSE:  0.007984432580058768  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2047  MSE:  0.007980082920306695  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2048  MSE:  0.007975751640801351  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2049  MSE:  0.007971438603321763  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2050  MSE:  0.007967143670566848  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2051  MSE:  0.00796286670615449  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2052  MSE:  0.007958607574618158  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2053  MSE:  0.007954366141407793  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2054  MSE:  0.007950142272884398  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2055  MSE:  0.007945935836323201  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2056  MSE:  0.00794174669990936  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2057  MSE:  0.00793757473273489  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2058  MSE:  0.007933419804802224  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2059  MSE:  0.00792928178701958  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2060  MSE:  0.00792516055119922  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2061  MSE:  0.007921055970057959  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2062  MSE:  0.007916967917215682  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2063  MSE:  0.00791289626719241  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2064  MSE:  0.00790884089541085  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2065  MSE:  0.007904801678190462  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2066  MSE:  0.007900778492750699  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2067  MSE:  0.00789677121720534  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2068  MSE:  0.007892779730566074  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2069  MSE:  0.007888803912737297  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2070  MSE:  0.007884843644517393  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2071  MSE:  0.007880898807595616  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2072  MSE:  0.007876969284552318  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2073  MSE:  0.007873054958856091  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2074  MSE:  0.00786915571486477  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2075  MSE:  0.00786527143782003  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2076  MSE:  0.007861402013849456  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2077  MSE:  0.0078575473299635  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2078  MSE:  0.007853707274053321  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2079  MSE:  0.007849881734891461  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2080  MSE:  0.007846070602127065  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2081  MSE:  0.007842273766285232  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2082  MSE:  0.007838491118765294  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2083  MSE:  0.007834722551839028  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2084  MSE:  0.007830967958649435  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2085  MSE:  0.007827227233205326  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2086  MSE:  0.00782350027038291  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2087  MSE:  0.007819786965922294  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2088  MSE:  0.007816087216423429  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2089  MSE:  0.007812400919345827  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2090  MSE:  0.007808727973004969  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2091  MSE:  0.007805068276569108  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2092  MSE:  0.007801421730060751  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2093  MSE:  0.00779778823434642  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2094  MSE:  0.007794167691139929  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2095  MSE:  0.007790560002998032  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2096  MSE:  0.007786965073314684  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2097  MSE:  0.007783382806322179  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2098  MSE:  0.00777981310708617  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2099  MSE:  0.0077762558814993155  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2100  MSE:  0.00777271103628251  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2101  MSE:  0.007769178478980665  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2102  MSE:  0.00776565811795651  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2103  MSE:  0.007762149862387681  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2104  MSE:  0.007758653622267311  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2105  MSE:  0.007755169308393678  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2106  MSE:  0.007751696832372111  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2107  MSE:  0.0077482361066058625  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2108  MSE:  0.007744787044297906  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2109  MSE:  0.00774134955944134  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2110  MSE:  0.007737923566819163  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2111  MSE:  0.0077345089819981965  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2112  MSE:  0.0077311057213252906  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2113  MSE:  0.007727713701922596  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2114  MSE:  0.007724332841684521  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2115  MSE:  0.007720963059271549  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2116  MSE:  0.007717604274106713  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2117  MSE:  0.007714256406370043  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2118  MSE:  0.007710919376995653  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2119  MSE:  0.007707593107665508  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2120  MSE:  0.007704277520805227  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2121  MSE:  0.007700972539579083  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2122  MSE:  0.007697678087885446  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2123  MSE:  0.007694394090351685  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2124  MSE:  0.007691120472329224  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2125  MSE:  0.00768785715988827  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2126  MSE:  0.007684604079813909  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2127  MSE:  0.007681361159599318  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2128  MSE:  0.00767812832744147  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2129  MSE:  0.007674905512236364  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2130  MSE:  0.007671692643573415  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2131  MSE:  0.00766848965173135  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2132  MSE:  0.0076652964676694494  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2133  MSE:  0.007662113023027925  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2134  MSE:  0.007658939250116282  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2135  MSE:  0.007655775081912544  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2136  MSE:  0.007652620452057413  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2137  MSE:  0.007649475294847097  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2138  MSE:  0.007646339545228066  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2139  MSE:  0.007643213138793431  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2140  MSE:  0.007640096011775776  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2141  MSE:  0.007636988101043457  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2142  MSE:  0.007633889344092756  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2143  MSE:  0.0076307996790451965  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2144  MSE:  0.007627719044639704  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2145  MSE:  0.007624647380228205  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2146  MSE:  0.007621584625770305  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2147  MSE:  0.00761853072182784  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2148  MSE:  0.007615485609558458  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2149  MSE:  0.0076124492307110565  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2150  MSE:  0.007609421527620641  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2151  MSE:  0.007606402443202174  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2152  MSE:  0.00760339192094455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2153  MSE:  0.00760038990490559  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2154  MSE:  0.007597396339709484  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2155  MSE:  0.007594411170535173  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2156  MSE:  0.007591434343116188  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2157  MSE:  0.007588465803735038  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2158  MSE:  0.007585505499213671  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2159  MSE:  0.0075825533769115485  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2160  MSE:  0.007579609384721322  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2161  MSE:  0.007576673471058877  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2162  MSE:  0.007573745584861872  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2163  MSE:  0.007570825675585143  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2164  MSE:  0.0075679136931910895  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2165  MSE:  0.007565009588148175  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2166  MSE:  0.007562113311424431  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2167  MSE:  0.007559224814482034  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2168  MSE:  0.007556344049273414  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2169  MSE:  0.00755347096823316  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2170  MSE:  0.007550605524276335  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2171  MSE:  0.007547747670791429  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2172  MSE:  0.007544897361636398  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2173  MSE:  0.007542054551130796  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2174  MSE:  0.007539219194055445  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2175  MSE:  0.007536391245644021  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2176  MSE:  0.007533570661577785  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2177  MSE:  0.007530757397984605  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2178  MSE:  0.007527951411427772  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2179  MSE:  0.007525152658907131  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2180  MSE:  0.007522361097852323  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2181  MSE:  0.00751957668611555  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2182  MSE:  0.007516799381969837  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2183  MSE:  0.00751402914410445  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2184  MSE:  0.007511265931616385  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2185  MSE:  0.007508509704010349  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2186  MSE:  0.007505760421192346  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2187  MSE:  0.007503018043464172  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2188  MSE:  0.007500282531520506  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2189  MSE:  0.007497553846443189  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2190  MSE:  0.007494831949696521  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2191  MSE:  0.007492116803125143  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2192  MSE:  0.007489408368947108  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2193  MSE:  0.007486706609750054  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2194  MSE:  0.007484011488489217  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2195  MSE:  0.007481322968479027  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2196  MSE:  0.007478641013393009  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2197  MSE:  0.007475965587257037  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2198  MSE:  0.007473296654446329  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2199  MSE:  0.0074706341796807125  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2200  MSE:  0.007467978128020623  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2201  MSE:  0.007465328464863887  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2202  MSE:  0.0074626851559417414  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2203  MSE:  0.007460048167312448  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2204  MSE:  0.007457417465360575  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2205  MSE:  0.0074547930167929415  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2206  MSE:  0.007452174788631444  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2207  MSE:  0.007449562748212954  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2208  MSE:  0.007446956863183892  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2209  MSE:  0.007444357101497722  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2210  MSE:  0.0074417634314083965  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2211  MSE:  0.007439175821470708  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2212  MSE:  0.0074365942405336906  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2213  MSE:  0.0074340186577384944  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2214  MSE:  0.007431449042515101  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2215  MSE:  0.007428885364577038  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2216  MSE:  0.007426327593920184  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2217  MSE:  0.007423775700818666  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2218  MSE:  0.007421229655820307  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2219  MSE:  0.007418689429744914  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2220  MSE:  0.007416154993680587  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2221  MSE:  0.0074136263189798265  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2222  MSE:  0.007411103377257247  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2223  MSE:  0.007408586140385877  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2224  MSE:  0.007406074580493717  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2225  MSE:  0.007403568669961095  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2226  MSE:  0.00740106838141893  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2227  MSE:  0.007398573687742873  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2228  MSE:  0.0073960845620526135  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2229  MSE:  0.007393600977708038  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2230  MSE:  0.00739112290830703  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2231  MSE:  0.007388650327680402  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2232  MSE:  0.007386183209893355  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2233  MSE:  0.007383721529238025  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2234  MSE:  0.007381265260233296  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2235  MSE:  0.007378814377622139  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2236  MSE:  0.007376368856367174  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2237  MSE:  0.007373928671650676  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2238  MSE:  0.007371493798868409  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2239  MSE:  0.007369064213632012  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2240  MSE:  0.00736663989176073  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2241  MSE:  0.007364220809282487  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2242  MSE:  0.007361806942431076  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2243  MSE:  0.007359398267642997  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2244  MSE:  0.00735699476155457  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2245  MSE:  0.007354596401001675  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2246  MSE:  0.007352203163013335  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2247  MSE:  0.007349815024814499  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2248  MSE:  0.007347431963819488  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2249  MSE:  0.007345053957632608  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2250  MSE:  0.00734268098404366  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2251  MSE:  0.007340313021027187  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2252  MSE:  0.007337950046739522  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2253  MSE:  0.007335592039516511  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2254  MSE:  0.007333238977873426  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2255  MSE:  0.0073308908404997935  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2256  MSE:  0.007328547606258656  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2257  MSE:  0.007326209254185394  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2258  MSE:  0.007323875763484784  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2259  MSE:  0.007321547113528049  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2260  MSE:  0.007319223283853957  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2261  MSE:  0.007316904254163195  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2262  MSE:  0.007314590004318526  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2263  MSE:  0.007312280514342626  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2264  MSE:  0.0073099757644162384  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2265  MSE:  0.007307675734876359  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2266  MSE:  0.007305380406213741  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2267  MSE:  0.007303089759071429  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2268  MSE:  0.007300803774244087  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2269  MSE:  0.007298522432674117  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2270  MSE:  0.007296245715452216  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2271  MSE:  0.007293973603814213  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2272  MSE:  0.007291706079138898  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2273  MSE:  0.007289443122949119  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2274  MSE:  0.007287184716906648  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2275  MSE:  0.007284930842811909  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2276  MSE:  0.007282681482605041  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2277  MSE:  0.007280436618359453  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2278  MSE:  0.007278196232283482  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2279  MSE:  0.0072759603067187525  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2280  MSE:  0.007273728824137843  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2281  MSE:  0.00727150176714231  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2282  MSE:  0.007269279118462964  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2283  MSE:  0.007267060860956596  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2284  MSE:  0.007264846977605604  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2285  MSE:  0.007262637451517273  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2286  MSE:  0.007260432265920247  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2287  MSE:  0.007258231404163927  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2288  MSE:  0.007256034849718905  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2289  MSE:  0.0072538425861736755  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2290  MSE:  0.007251654597234249  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2291  MSE:  0.007249470866721861  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2292  MSE:  0.00724729137857274  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2293  MSE:  0.007245116116836693  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2294  MSE:  0.007242945065675088  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2295  MSE:  0.0072407782093609424  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2296  MSE:  0.007238615532275836  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2297  MSE:  0.0072364570189106855  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2298  MSE:  0.007234302653863566  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2299  MSE:  0.007232152421838653  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2300  MSE:  0.0072300063076445745  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2301  MSE:  0.007227864296194103  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2302  MSE:  0.0072257263725031134  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2303  MSE:  0.007223592521688781  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2304  MSE:  0.007221462728968954  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2305  MSE:  0.0072193369796603925  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2306  MSE:  0.007217215259178991  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2307  MSE:  0.00721509755303731  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2308  MSE:  0.007212983846845041  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2309  MSE:  0.007210874126306285  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2310  MSE:  0.007208768377219945  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2311  MSE:  0.007206666585477673  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2312  MSE:  0.0072045687370642176  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2313  MSE:  0.007202474818055829  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2314  MSE:  0.007200384814618287  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2315  MSE:  0.007198298713006975  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2316  MSE:  0.007196216499566086  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2317  MSE:  0.0071941381607280135  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2318  MSE:  0.007192063683010763  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2319  MSE:  0.007189993053019137  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2320  MSE:  0.00718792625744155  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2321  MSE:  0.007185863283052962  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2322  MSE:  0.007183804116708884  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2323  MSE:  0.007181748745348455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2324  MSE:  0.007179697155992474  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2325  MSE:  0.0071776493357419945  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2326  MSE:  0.007175605271778315  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2327  MSE:  0.007173564951362194  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2328  MSE:  0.007171528361831839  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2329  MSE:  0.0071694954906041915  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2330  MSE:  0.007167466325171571  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2331  MSE:  0.007165440853103595  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2332  MSE:  0.0071634190620439376  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2333  MSE:  0.00716140093971215  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2334  MSE:  0.007159386473900096  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2335  MSE:  0.007157375652474502  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2336  MSE:  0.007155368463372702  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2337  MSE:  0.007153364894605184  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2338  MSE:  0.0071513649342512435  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2339  MSE:  0.007149368570463831  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2340  MSE:  0.007147375791462451  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2341  MSE:  0.00714538658553659  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2342  MSE:  0.007143400941044757  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2343  MSE:  0.007141418846412207  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2344  MSE:  0.007139440290130925  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2345  MSE:  0.0071374652607607645  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2346  MSE:  0.007135493746925583  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2347  MSE:  0.007133525737315045  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2348  MSE:  0.007131561220683997  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2349  MSE:  0.0071296001858500695  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2350  MSE:  0.007127642621695202  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2351  MSE:  0.007125688517162921  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2352  MSE:  0.007123737861259537  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2353  MSE:  0.007121790643052927  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2354  MSE:  0.007119846851672905  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2355  MSE:  0.00711790647630788  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2356  MSE:  0.00711596950620676  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2357  MSE:  0.007114035930679291  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2358  MSE:  0.0071121057390916235  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2359  MSE:  0.007110178920870589  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2360  MSE:  0.007108255465498807  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2361  MSE:  0.0071063353625167615  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2362  MSE:  0.00710441860152204  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2363  MSE:  0.007102505172168929  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2364  MSE:  0.00710059506416617  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2365  MSE:  0.007098688267278333  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2366  MSE:  0.007096784771324886  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2367  MSE:  0.007094884566179062  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2368  MSE:  0.00709298764176823  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2369  MSE:  0.007091093988074211  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2370  MSE:  0.007089203595128759  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2371  MSE:  0.007087316453019108  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2372  MSE:  0.0070854325518826195  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2373  MSE:  0.0070835518819082595  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2374  MSE:  0.007081674433337136  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2375  MSE:  0.007079800196460241  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2376  MSE:  0.007077929161618314  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2377  MSE:  0.007076061319202804  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2378  MSE:  0.00707419665965365  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2379  MSE:  0.0070723351734603875  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2380  MSE:  0.007070476851160761  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2381  MSE:  0.007068621683340326  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2382  MSE:  0.007066769660633331  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2383  MSE:  0.0070649207737200495  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2384  MSE:  0.007063075013328038  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2385  MSE:  0.007061232370232296  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2386  MSE:  0.007059392835253148  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2387  MSE:  0.007057556399256418  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2388  MSE:  0.007055723053153992  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2389  MSE:  0.007053892787902084  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2390  MSE:  0.007052065594502806  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2391  MSE:  0.0070502414640012095  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2392  MSE:  0.007048420387486889  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2393  MSE:  0.0070466023560930595  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2394  MSE:  0.007044787360996252  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2395  MSE:  0.0070429753934148406  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2396  MSE:  0.007041166444611728  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2397  MSE:  0.007039360505889904  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2398  MSE:  0.007037557568596092  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2399  MSE:  0.007035757624117598  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2400  MSE:  0.007033960663882205  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2401  MSE:  0.007032166679360283  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2402  MSE:  0.0070303756620617155  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2403  MSE:  0.007028587603536925  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2404  MSE:  0.007026802495376196  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2405  MSE:  0.007025020329209009  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2406  MSE:  0.007023241096705484  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2407  MSE:  0.0070214647895741585  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2408  MSE:  0.00701969139956145  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2409  MSE:  0.007017920918452695  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2410  MSE:  0.007016153338072325  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2411  MSE:  0.0070143886502817  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2412  MSE:  0.0070126268469792915  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2413  MSE:  0.007010867920102135  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2414  MSE:  0.007009111861624176  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2415  MSE:  0.00700735866355491  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2416  MSE:  0.007005608317941032  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2417  MSE:  0.007003860816865357  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2418  MSE:  0.007002116152447179  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2419  MSE:  0.007000374316840716  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2420  MSE:  0.0069986353022356285  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2421  MSE:  0.006996899100856867  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2422  MSE:  0.006995165704964776  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2423  MSE:  0.006993435106853519  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2424  MSE:  0.00699170729885214  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2425  MSE:  0.006989982273323877  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2426  MSE:  0.006988260022665396  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2427  MSE:  0.006986540539307664  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2428  MSE:  0.006984823815714183  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2429  MSE:  0.00698310984438223  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2430  MSE:  0.006981398617842433  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2431  MSE:  0.006979690128657126  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2432  MSE:  0.006977984369421858  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2433  MSE:  0.006976281332763854  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2434  MSE:  0.006974581011343138  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2435  MSE:  0.006972883397850785  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2436  MSE:  0.006971188485009027  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2437  MSE:  0.006969496265573515  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2438  MSE:  0.006967806732328812  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2439  MSE:  0.006966119878091102  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2440  MSE:  0.006964435695707752  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2441  MSE:  0.006962754178056034  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2442  MSE:  0.006961075318043973  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2443  MSE:  0.00695939910860953  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2444  MSE:  0.006957725542720102  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2445  MSE:  0.00695605461337345  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2446  MSE:  0.006954386313596824  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2447  MSE:  0.006952720636445614  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2448  MSE:  0.006951057575005515  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2449  MSE:  0.0069493971223911636  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2450  MSE:  0.006947739271744598  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2451  MSE:  0.006946084016238044  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2452  MSE:  0.0069444313490700455  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2453  MSE:  0.006942781263469329  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2454  MSE:  0.006941133752691752  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2455  MSE:  0.006939488810020508  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2456  MSE:  0.0069378464287661165  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2457  MSE:  0.006936206602267865  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2458  MSE:  0.006934569323891354  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2459  MSE:  0.006932934587029667  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2460  MSE:  0.006931302385101623  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2461  MSE:  0.006929672711554518  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2462  MSE:  0.006928045559860534  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2463  MSE:  0.006926420923518271  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2464  MSE:  0.006924798796054245  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2465  MSE:  0.006923179171019828  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2466  MSE:  0.006921562041991418  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2467  MSE:  0.00691994740257248  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2468  MSE:  0.00691833524639188  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2469  MSE:  0.006916725567103475  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2470  MSE:  0.0069151183583856905  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2471  MSE:  0.00691351361394247  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2472  MSE:  0.006911911327504214  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2473  MSE:  0.006910311492822799  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2474  MSE:  0.0069087141036779055  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2475  MSE:  0.006907119153872482  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2476  MSE:  0.006905526637233293  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2477  MSE:  0.006903936547610427  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2478  MSE:  0.006902348878880407  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2479  MSE:  0.0069007636249415385  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2480  MSE:  0.006899180779717421  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2481  MSE:  0.00689760033715315  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2482  MSE:  0.006896022291219164  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2483  MSE:  0.006894446635908622  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2484  MSE:  0.006892873365237205  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2485  MSE:  0.006891302473243847  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2486  MSE:  0.006889733953990366  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2487  MSE:  0.006888167801562139  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2488  MSE:  0.006886604010065745  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2489  MSE:  0.006885042573631046  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2490  MSE:  0.006883483486410064  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2491  MSE:  0.006881926742577287  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2492  MSE:  0.006880372336327367  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2493  MSE:  0.0068788202618801095  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2494  MSE:  0.006877270513475245  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2495  MSE:  0.006875723085373605  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2496  MSE:  0.006874177971858752  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2497  MSE:  0.006872635167234886  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2498  MSE:  0.006871094665828137  Learning Rate:  0.6400000000000001 \n",
      "\n",
      "Epoch:  2499  MSE:  0.006869556461985039  Learning Rate:  0.6400000000000001 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "eta = 4 # Learning Rate\n",
    "W_Layer_2 = W_Layer_2_Guess\n",
    "W_Layer_3 = W_Layer_3_Guess\n",
    "\n",
    "\n",
    "MSE_vec = np.zeros(int(max_iter)+1) # Mean Squared Error\n",
    "MSE_vec[0] = 1\n",
    "\n",
    "iterations = 1\n",
    "\n",
    "while iterations < max_iter:\n",
    "    \n",
    "    # update the weights for each sample in training set (online learning)\n",
    "    for i in range(0, n):\n",
    "        \n",
    "        x = np.array([X[:,i]]).T\n",
    "        \n",
    "        [v_2, v_3, y_2, y_3] = ForwardPass(x, W_Layer_2, W_Layer_3)\n",
    "        \n",
    "        err = FeedbackError(y_3, D[i])\n",
    "        \n",
    "        [Gradient_Layer_2, Gradient_Layer_3] =BackwardPass(err, x, v_2, v_3, y_2, y_3, W_Layer_3)\n",
    "        \n",
    "        #[W_Layer_2, W_Layer_3] = UpdateWeights(eta, Gradient_Layer_2, Gradient_Layer_3, W_Layer_2, W_Layer_3)\n",
    "        [W_Layer_2, W_Layer_3, M2, M3] = UpdateWeights_withMomentum(eta, beta, M2, M3, Gradient_Layer_2, Gradient_Layer_3, W_Layer_2, W_Layer_3)\n",
    "\n",
    "    # use the latest weights to obtain outputs for all the training set\n",
    "    [v_2, v_3, y_2, y_3] = ForwardPass(X, W_Layer_2, W_Layer_3)\n",
    "    \n",
    "    # calculate the MSE for the current epoch\n",
    "    MSE_vec[iterations] = CalculateMSE(X, D, y_3)\n",
    "    \n",
    "    print('Epoch: ', iterations, ' MSE: ', MSE_vec[iterations], ' Learning Rate: ', eta, '\\n')\n",
    "    \n",
    "    if MSE_vec[iterations] < tol:\n",
    "        print('Optimal Weights Reached')\n",
    "        break\n",
    "    \n",
    "    eta = CheckLearningrate(eta, MSE_vec, iterations)\n",
    "    iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot\n",
    "# Plot 1\n",
    "fig, ax1 = plt.subplots()  \n",
    "[v_2, v_3, y_2, y_3] = ForwardPass(X, W_Layer_2, W_Layer_3)    \n",
    "ax1.plot(X[1,:].T, D, 'b.', label='Actual Curve')\n",
    "ax1.plot(X[1,:].T, y_3, 'r.', label='Curve Fit')\n",
    "plt.title(r'Curve Fitting for $ y = \\sin{20x} + 3 x + v $ where $v \\sim Unif[-0.1, 0.1] $', fontsize=12)\n",
    "plt.xlabel(r'X', fontsize=12)\n",
    "plt.ylabel(r'Y', fontsize=12)\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('actualCurveVsFit_momentum.png', dpi = 450)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot 2\n",
    "fig, ax2 = plt.subplots()\n",
    "ax2.plot(np.linspace(1, iterations-1, num=iterations-1), MSE_vec[1:int(max_iter)], label='Mean Squared Error')\n",
    "#plt.title('No of Training Iterations VS Mean Squared Error (MSE)')\n",
    "plt.xlabel(r'Epoch', fontsize=12)\n",
    "plt.ylabel(r'MSE', fontsize=12)\n",
    "plt.legend()\n",
    "plt.ylim([0, 0.8])\n",
    "plt.savefig('MSEVsIter_momentum.png', dpi = 450)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
